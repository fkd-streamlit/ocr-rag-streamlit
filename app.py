# app.py  (Route A: ãƒ­ãƒ¼ã‚«ãƒ«ã§OCRâ†’JSONåŒ–â†’æ¤œç´¢) / Streamlit
# - data/ocr_results/*.json ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿ã—ã¦æ¤œç´¢
# - ã‚¯ã‚¨ãƒªã«å¿œã˜ã¦çµæœãŒå¤‰ã‚ã‚‹ï¼ˆãƒã‚°ä¿®æ­£ç‰ˆï¼‰
# - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(BM25é¢¨) + TF-IDF ã®ä¸¡å¯¾å¿œï¼ˆscikit-learnä½¿ç”¨ï¼‰
#
# æ³¨æ„: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯Streamlit Cloudã§å®Ÿè¡Œã•ã‚Œã¾ã™
# - pdf2imageã‚„pytesseractã¯ä½¿ç”¨ã—ã¾ã›ã‚“ï¼ˆOCRã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œï¼‰
# - å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª: streamlit, scikit-learn ã®ã¿

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Tuple

import streamlit as st


# ----------------------------
# è¨­å®š
# ----------------------------
OCR_RESULTS_DIR = Path("data") / "ocr_results"

DEFAULT_TOPK = 5

# å®¹é‡ä¸Šé™è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
# config.pyã‹ã‚‰èª­ã¿è¾¼ã‚‚ã†ã¨ã—ã¾ã™ãŒã€å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
MAX_JSON_FILE_SIZE_MB = 100
MAX_TOTAL_CHUNKS = 50000

# config.pyãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ä¸Šæ›¸ãï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import config
    MAX_JSON_FILE_SIZE_MB = getattr(config, 'MAX_JSON_FILE_SIZE_MB', MAX_JSON_FILE_SIZE_MB)
    MAX_TOTAL_CHUNKS = getattr(config, 'MAX_TOTAL_CHUNKS', MAX_TOTAL_CHUNKS)
except (ImportError, AttributeError, Exception):
    # config.pyãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    pass


# ----------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------
def normalize_text(s: str) -> str:
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def simple_tokenize_ja(s: str) -> List[str]:
    """
    å½¢æ…‹ç´ è§£æãªã—ã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã€‚
    - æ—¥æœ¬èª/è‹±æ•°å­—ã‚’ãã‚Œã£ã½ãåˆ†å‰²ã—ã¦ã€TF-IDFã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®å…¥åŠ›ã«ã™ã‚‹
    """
    s = normalize_text(s).lower()
    # ã²ã‚‰ãŒãª/ã‚«ã‚¿ã‚«ãƒŠ/æ¼¢å­—/è‹±æ•°å­—ã‚’ã¾ã¨ã‚ã¦æ‹¾ã†
    tokens = re.findall(r"[ä¸€-é¾¥]+|[ã-ã‚“]+|[ã‚¡-ãƒ´ãƒ¼]+|[a-z0-9]+", s)
    # 1æ–‡å­—ã ã‘ã¯ãƒã‚¤ã‚ºã«ãªã‚Šã‚„ã™ã„ã®ã§é™¤å¤–ï¼ˆå¿…è¦ãªã‚‰å¤–ã—ã¦ãã ã•ã„ï¼‰
    tokens = [t for t in tokens if len(t) >= 2]
    return tokens


def load_ocr_json(path: Path) -> List[Dict[str, Any]]:
    """
    local_ocr_to_json.py ãŒå‡ºåŠ›ã™ã‚‹æƒ³å®šã®JSON:
    { "meta":..., "pages":[{"page":1,"text":"..."}, ...] }
    ã‚‚ã—å½¢å¼ãŒé•ã£ã¦ã‚‚ã€pages/text ã‚’ã§ãã‚‹ã ã‘æ‹¾ã†
    """
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    file_size_mb = path.stat().st_size / (1024 * 1024)
    if file_size_mb > MAX_JSON_FILE_SIZE_MB:
        raise ValueError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™: {path.name} ({file_size_mb:.1f}MB > {MAX_JSON_FILE_SIZE_MB}MB)")
    
    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {path.name} - {e}")
    except MemoryError:
        raise MemoryError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¦ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {path.name} ({file_size_mb:.1f}MB)")

    pages = []
    if isinstance(data, dict) and "pages" in data and isinstance(data["pages"], list):
        for p in data["pages"]:
            txt = p.get("text", "")
            page_no = p.get("page", None)
            pages.append({"page": page_no, "text": normalize_text(txt)})
        return pages

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæƒ³å®šå¤–å½¢å¼ï¼‰
    if isinstance(data, list):
        for i, p in enumerate(data, start=1):
            if isinstance(p, dict) and "text" in p:
                pages.append({"page": p.get("page", i), "text": normalize_text(p.get("text", ""))})
    return pages


@dataclass
class Chunk:
    doc_id: str
    source_file: str
    page: int | None
    chunk_id: int
    text: str


def make_chunks(doc_id: str, source_file: str, pages: List[Dict[str, Any]], chunk_size: int = 900, overlap: int = 150) -> List[Chunk]:
    chunks: List[Chunk] = []
    cid = 0
    for p in pages:
        page_no = p.get("page", None)
        text = p.get("text", "")
        if not text:
            continue

        # ãƒšãƒ¼ã‚¸ã”ã¨ã«ã‚¹ãƒ©ã‚¤ãƒ‰ãƒãƒ£ãƒ³ã‚¯
        start = 0
        while start < len(text):
            end = min(len(text), start + chunk_size)
            ctext = text[start:end]
            chunks.append(
                Chunk(
                    doc_id=doc_id,
                    source_file=source_file,
                    page=page_no,
                    chunk_id=cid,
                    text=ctext,
                )
            )
            cid += 1
            if end == len(text):
                break
            start = max(0, end - overlap)
    return chunks


def load_all_chunks(ocr_dir: Path) -> List[Chunk]:
    chunks: List[Chunk] = []
    if not ocr_dir.exists():
        return chunks

    json_files = sorted(ocr_dir.glob("*.json"))
    if not json_files:
        return chunks
    
    skipped_files = []
    for jp in json_files:
        try:
            pages = load_ocr_json(jp)
            doc_id = jp.stem
            new_chunks = make_chunks(doc_id=doc_id, source_file=jp.name, pages=pages)
            
            # ç·ãƒãƒ£ãƒ³ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
            if len(chunks) + len(new_chunks) > MAX_TOTAL_CHUNKS:
                skipped_files.append(f"{jp.name} (ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šé™ã«é”ã—ã¾ã—ãŸ: {len(chunks) + len(new_chunks)} > {MAX_TOTAL_CHUNKS})")
                break
            
            chunks.extend(new_chunks)
        except (ValueError, MemoryError) as e:
            skipped_files.append(f"{jp.name} ({str(e)})")
            continue
    
    if skipped_files:
        import warnings
        warnings.warn(f"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ:\n" + "\n".join(f"  - {f}" for f in skipped_files))
    
    return chunks


# ----------------------------
# æ¤œç´¢ï¼ˆ1ï¼‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“BM25é¢¨ï¼‰
# ----------------------------
def keyword_score(query: str, text: str) -> float:
    q_tokens = simple_tokenize_ja(query)
    if not q_tokens:
        return 0.0
    t = normalize_text(text).lower()
    score = 0.0
    for tok in q_tokens:
        # å‡ºç¾å›æ•°ã‚’åŠ ç‚¹ï¼ˆè»½ã„é‡ã¿ï¼‰
        c = t.count(tok)
        if c > 0:
            score += 1.0 + min(3.0, c * 0.3)
    return score


# ----------------------------
# æ¤œç´¢ï¼ˆ2ï¼‰TF-IDF
# ----------------------------
@st.cache_resource(show_spinner=False)
def build_tfidf_index(texts: List[str]):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    vectorizer = TfidfVectorizer(
        tokenizer=simple_tokenize_ja,
        lowercase=True,
        min_df=1,
    )
    X = vectorizer.fit_transform(texts)

    def search(q: str) -> List[float]:
        qv = vectorizer.transform([q])
        sims = cosine_similarity(qv, X).flatten()
        return sims.tolist()

    return search


def search_chunks(chunks: List[Chunk], query: str, topk: int = 5) -> List[Tuple[float, Chunk]]:
    if not chunks:
        return []

    texts = [c.text for c in chunks]
    tfidf_search = build_tfidf_index(texts)
    tfidf_scores = tfidf_search(query)

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚æ··ãœã¦æœ€çµ‚ã‚¹ã‚³ã‚¢
    scored: List[Tuple[float, Chunk]] = []
    for s, c in zip(tfidf_scores, chunks):
        ks = keyword_score(query, c.text)
        final = float(s) * 1.0 + ks * 0.25  # â†æ··åˆæ¯”ç‡ï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰
        scored.append((final, c))

    scored.sort(key=lambda x: x[0], reverse=True)
    # ã‚¹ã‚³ã‚¢ãŒã»ã¼ã‚¼ãƒ­ã®ã‚‚ã®ã¯é™¤å¤–ï¼ˆãŸã ã—å…¨éƒ¨ã‚¼ãƒ­ãªã‚‰ãƒˆãƒƒãƒ—ã‚’è¿”ã™ï¼‰
    if scored and scored[0][0] <= 1e-8:
        return scored[:topk]
    return [x for x in scored[:topk] if x[0] > 1e-8] or scored[:topk]


# ----------------------------
# UI
# ----------------------------
st.set_page_config(page_title="OCR RAG (Local)", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ OCR RAGï¼ˆãƒ­ãƒ¼ã‚«ãƒ«OCRâ†’JSONâ†’æ¤œç´¢ï¼‰")

with st.sidebar:
    st.header("è¨­å®š")
    st.write("æ¤œç´¢å¯¾è±¡ï¼š `data/ocr_results/*.json`")
    topk = st.slider("è¡¨ç¤ºä»¶æ•° (Top-K)", 1, 10, DEFAULT_TOPK)
    chunk_size = st.number_input("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", min_value=300, max_value=3000, value=900, step=100)
    overlap = st.number_input("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", min_value=0, max_value=500, value=150, step=10)
    reload_btn = st.button("ğŸ”„ JSONã‚’å†èª­ã¿è¾¼ã¿")

# èª­ã¿è¾¼ã¿
if "chunks" not in st.session_state or reload_btn:
    with st.spinner("JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        try:
            raw_chunks = load_all_chunks(OCR_RESULTS_DIR)

            # ãƒãƒ£ãƒ³ã‚¯è¨­å®šå¤‰æ›´ã«å¯¾å¿œï¼ˆå†åˆ†å‰²ã—ãŸã„ï¼‰
            # ã„ã£ãŸã‚“ pages ã‚’èª­ã¿ç›´ã—ã¦ä½œã‚Šç›´ã™
            rebuilt: List[Chunk] = []
            skipped_count = 0
            
            json_files = sorted(OCR_RESULTS_DIR.glob("*.json"))
            progress_bar = st.progress(0)
            for idx, jp in enumerate(json_files):
                try:
                    pages = load_ocr_json(jp)
                    new_chunks = make_chunks(jp.stem, jp.name, pages, chunk_size=int(chunk_size), overlap=int(overlap))
                    
                    # ç·ãƒãƒ£ãƒ³ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
                    if len(rebuilt) + len(new_chunks) > MAX_TOTAL_CHUNKS:
                        st.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šé™ã«é”ã—ãŸãŸã‚ã€{jp.name} ä»¥é™ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                        skipped_count = len(json_files) - idx
                        break
                    
                    rebuilt.extend(new_chunks)
                    progress_bar.progress((idx + 1) / len(json_files))
                except (ValueError, MemoryError) as e:
                    st.warning(f"âš ï¸ {jp.name} ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {e}")
                    skipped_count += 1
                    continue
            
            progress_bar.empty()
            st.session_state["chunks"] = rebuilt
            
            if skipped_count > 0:
                st.info(f"â„¹ï¸ {skipped_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚å®¹é‡ä¸Šé™ã‚’èª¿æ•´ã™ã‚‹å ´åˆã¯ config.py ã‚’ç·¨é›†ã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.session_state["chunks"] = []

chunks: List[Chunk] = st.session_state["chunks"]

st.caption(f"èª­ã¿è¾¼ã¿JSONæ•°: {len(list(OCR_RESULTS_DIR.glob('*.json')))} / ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

if not chunks:
    st.warning("`data/ocr_results` ã« JSON ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãš local_ocr_to_json.py ã§PDF/ç”»åƒã‚’JSONåŒ–ã—ã¦å…¥ã‚Œã¦ãã ã•ã„ã€‚")
    st.stop()

query = st.text_input("æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šææ–™ / å®šå“¡ / 5052 / ã‚¢ãƒ«ãƒŸãƒ‹ã‚¦ãƒ ï¼‰", value="å®šå“¡")
go = st.button("ğŸ” æ¤œç´¢")

if go:
    results = search_chunks(chunks, query=query, topk=topk)

    st.subheader("æ¤œç´¢çµæœ")
    if not results:
        st.info("è©²å½“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¹ã‚³ã‚¢ãŒå…¨ã¦ã‚¼ãƒ­ï¼‰ã€‚åˆ¥ã®èªã‚„è¡¨è¨˜ã‚†ã‚Œã§è©¦ã—ã¦ãã ã•ã„ã€‚")
    else:
        for score, c in results:
            header = f"Score: {score:.4f} | File: {c.source_file} | Doc: {c.doc_id}"
            if c.page is not None:
                header += f" | Page: {c.page}"
            with st.expander(header, expanded=False):
                # ã‚¯ã‚¨ãƒªã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                t = c.text
                if query.strip():
                    t = re.sub(re.escape(query.strip()), lambda m: f"**{m.group(0)}**", t)
                st.write(t)


