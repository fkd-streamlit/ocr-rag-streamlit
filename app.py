# app.py
# -----------------------------------------------
# OCR â†’ ãƒ†ã‚­ã‚¹ãƒˆåŒ– â†’ æ¤œç´¢ï¼ˆTF-IDFï¼‰ã¾ã§ã‚’ â€œStreamlit Cloud å˜ä½“â€ ã§å®Œçµã•ã›ã‚‹ç‰ˆ
# - ãƒ¦ãƒ¼ã‚¶ãƒ¼å´ã« Tesseract / Poppler ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼ˆCloud å´ã§ packages.txt ã§å…¥ã‚Œã‚‹æƒ³å®šï¼‰
# - PDF / ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ OCR â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ æ¤œç´¢
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ OCR ç²¾åº¦ï¼ˆDPI/PSM/OEM/å‰å‡¦ç†ï¼‰ã‚’èª¿æ•´å¯èƒ½
# -----------------------------------------------

from __future__ import annotations

import io
import json
import math
import os
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

# ç”»åƒå‡¦ç†
import cv2
from PIL import Image

# OCR / PDF
import pytesseract
from pdf2image import convert_from_bytes

# æ¤œç´¢ï¼ˆTF-IDFï¼‰
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# =========================
# åŸºæœ¬è¨­å®š
# =========================
APP_TITLE = "OCR RAG Searchï¼ˆPDF/ç”»åƒ â†’ OCR â†’ æ¤œç´¢ï¼‰"
DEFAULT_LANG = "jpn+eng"  # jpnã®ã¿ã§ã‚‚å¯
DEFAULT_DPI = 250
DEFAULT_PSM = 6
DEFAULT_OEM = 3

# JSONï¼ˆOCRçµæœï¼‰ã®ä¿å­˜å…ˆï¼ˆCloudã§ã‚‚å‹•ãã‚ˆã†ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
DEFAULT_JSON_DIR = os.path.join("data", "ocr_results")


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def ensure_dirs() -> None:
    os.makedirs(DEFAULT_JSON_DIR, exist_ok=True)


def set_tesseract_cmd_if_needed() -> None:
    """
    Windowsãƒ­ãƒ¼ã‚«ãƒ«ç”¨ï¼šTESSERACT_CMD ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ã†ã€‚
    Streamlit Cloud ã§ã¯åŸºæœ¬ PATH ã« tesseract ãŒå…¥ã‚‹æƒ³å®šã€‚
    """
    cmd = os.environ.get("TESSERACT_CMD", "").strip()
    if cmd:
        pytesseract.pytesseract.tesseract_cmd = cmd


def is_tesseract_available() -> Tuple[bool, str]:
    """
    tesseract ãŒä½¿ãˆã‚‹ã‹è»½ããƒã‚§ãƒƒã‚¯
    """
    try:
        v = pytesseract.get_tesseract_version()
        return True, f"Tesseract: {v}"
    except Exception as e:
        return False, f"TesseractãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}"


def safe_filename(name: str) -> str:
    name = name.strip().replace("\\", "_").replace("/", "_")
    name = re.sub(r"[^\w\-\.\(\)ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]+", "_", name)
    return name[:120] if len(name) > 120 else name


def pil_to_cv(img: Image.Image) -> np.ndarray:
    arr = np.array(img.convert("RGB"))
    return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)


def cv_to_pil(img_bgr: np.ndarray) -> Image.Image:
    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    return Image.fromarray(rgb)


def preprocess_for_ocr(
    img_bgr: np.ndarray,
    *,
    scale: float = 1.5,
    denoise: int = 1,
    contrast: int = 20,   # -100..100
    brightness: int = 0,  # -100..100
    binarize: bool = True,
    adaptive: bool = True,
    invert: bool = False,
    sharpen: bool = True,
) -> np.ndarray:
    """
    æ‰‹æ›¸ã/æ¿ƒæ·¡/ã‚¹ã‚­ãƒ£ãƒ³ã®ãƒ–ãƒ¬ã«å¯¾å¿œã—ã‚„ã™ã„å‰å‡¦ç†ã‚»ãƒƒãƒˆ
    """
    h, w = img_bgr.shape[:2]
    if scale and scale != 1.0:
        img_bgr = cv2.resize(img_bgr, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_CUBIC)

    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

    # æ˜ã‚‹ã•ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
    # new = gray * (1 + contrast/100) + brightness
    alpha = 1.0 + (contrast / 100.0)
    beta = brightness
    gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)

    if denoise >= 1:
        # è»½ã„ãƒã‚¤ã‚ºé™¤å»
        gray = cv2.medianBlur(gray, 3)
    if denoise >= 2:
        # ã‚‚ã†å°‘ã—å¼·ã‚
        gray = cv2.bilateralFilter(gray, 7, 50, 50)

    if sharpen:
        kernel = np.array([[0, -1, 0],
                           [-1, 5, -1],
                           [0, -1, 0]], dtype=np.float32)
        gray = cv2.filter2D(gray, -1, kernel)

    if binarize:
        if adaptive:
            th = cv2.adaptiveThreshold(
                gray, 255,
                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                cv2.THRESH_BINARY,
                31, 10
            )
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        gray = th

    if invert:
        gray = cv2.bitwise_not(gray)

    # pytesseract ã¯ PIL ã‚‚å—ã‘ã‚‰ã‚Œã‚‹ãŒã€ã“ã“ã§ã¯ BGR ã«æˆ»ã™ï¼ˆè¡¨ç¤ºã«ã‚‚ä½¿ãˆã‚‹ï¼‰
    out_bgr = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
    return out_bgr


def run_tesseract(
    img_bgr: np.ndarray,
    *,
    lang: str,
    psm: int,
    oem: int,
) -> str:
    pil = cv_to_pil(img_bgr)
    config = f"--oem {oem} --psm {psm}"
    text = pytesseract.image_to_string(pil, lang=lang, config=config)
    return text.strip()


def pdf_to_images(pdf_bytes: bytes, dpi: int) -> List[Image.Image]:
    """
    Poppler ãŒå…¥ã£ã¦ã„ã‚Œã° convert_from_bytes ãŒå‹•ãï¼ˆStreamlit Cloud ã§ã¯ packages.txt ã§å°å…¥æƒ³å®šï¼‰
    """
    images = convert_from_bytes(pdf_bytes, dpi=dpi)
    return images


def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:
    text = re.sub(r"\s+\n", "\n", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    if not text:
        return []
    chunks = []
    i = 0
    n = len(text)
    while i < n:
        j = min(i + chunk_size, n)
        chunk = text[i:j].strip()
        if chunk:
            chunks.append(chunk)
        if j >= n:
            break
        i = max(0, j - overlap)
    return chunks


def highlight_snippet(s: str, q: str, max_len: int = 350) -> str:
    s2 = " ".join(s.split())
    q = q.strip()
    if not q:
        return (s2[:max_len] + "â€¦") if len(s2) > max_len else s2

    # ã‚¯ã‚¨ãƒªã¯ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®èªã‚‚æ‹¾ã†
    terms = [t for t in re.split(r"\s+", q) if t]
    # ã¾ãšæœ€åˆã®ä¸€è‡´ä½ç½®ã‚’æ¢ã™
    pos = None
    for t in terms:
        m = re.search(re.escape(t), s2, flags=re.IGNORECASE)
        if m:
            pos = m.start()
            break

    if pos is None:
        return (s2[:max_len] + "â€¦") if len(s2) > max_len else s2

    start = max(0, pos - max_len // 3)
    end = min(len(s2), start + max_len)
    snippet = s2[start:end]
    if start > 0:
        snippet = "â€¦" + snippet
    if end < len(s2):
        snippet = snippet + "â€¦"

    # å¼·èª¿ï¼ˆå¤ªå­—ï¼‰â€»markdown
    for t in sorted(terms, key=len, reverse=True):
        snippet = re.sub(
            re.escape(t),
            lambda m: f"**{m.group(0)}**",
            snippet,
            flags=re.IGNORECASE,
        )
    return snippet


# =========================
# TF-IDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
# =========================
@dataclass
class IndexedChunk:
    doc_name: str
    page: int
    chunk_id: int
    text: str


class TfIdfSearchIndex:
    def __init__(self) -> None:
        self.vectorizer: Optional[TfidfVectorizer] = None
        self.matrix = None
        self.items: List[IndexedChunk] = []

    def build(self, items: List[IndexedChunk]) -> None:
        self.items = items
        corpus = [it.text for it in items]
        self.vectorizer = TfidfVectorizer(
            lowercase=False,          # æ—¥æœ¬èªã®ãŸã‚
            token_pattern=r"(?u)\b\w+\b",
            ngram_range=(1, 2),
            max_features=80000,
        )
        self.matrix = self.vectorizer.fit_transform(corpus)

    def search(self, query: str, top_k: int = 8) -> List[Tuple[IndexedChunk, float]]:
        if not self.items or self.vectorizer is None or self.matrix is None:
            return []
        q = query.strip()
        if not q:
            return []
        qv = self.vectorizer.transform([q])
        sims = cosine_similarity(qv, self.matrix).flatten()
        if sims.size == 0:
            return []
        idxs = np.argsort(-sims)[:top_k]
        results = [(self.items[i], float(sims[i])) for i in idxs]
        return results


# =========================
# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå½¢å¼ï¼ˆJSONï¼‰
# =========================
def make_doc_json(
    doc_name: str,
    pages_text: List[str],
    meta: Dict[str, Any],
) -> Dict[str, Any]:
    return {
        "schema": "ocr_doc_v1",
        "doc_name": doc_name,
        "meta": meta,
        "pages": [{"page": i + 1, "text": pages_text[i]} for i in range(len(pages_text))],
    }


def load_doc_json(file_bytes: bytes) -> Dict[str, Any]:
    return json.loads(file_bytes.decode("utf-8"))


def doc_to_index_items(doc: Dict[str, Any], chunk_size: int, overlap: int) -> List[IndexedChunk]:
    doc_name = doc.get("doc_name", "document")
    items: List[IndexedChunk] = []
    for p in doc.get("pages", []):
        page_no = int(p.get("page", 0))
        text = p.get("text", "") or ""
        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)
        for ci, ch in enumerate(chunks):
            items.append(IndexedChunk(doc_name=doc_name, page=page_no, chunk_id=ci, text=ch))
    return items


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ§ ", layout="wide")
ensure_dirs()
set_tesseract_cmd_if_needed()

st.title(APP_TITLE)
st.caption("PDF/ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ OCR â†’ ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã€‚Cloudã§å…±æœ‰ã™ã‚‹å‰æã®æ§‹æˆã§ã™ã€‚")

ok, tmsg = is_tesseract_available()
colA, colB = st.columns([1, 2])
with colA:
    st.write("**ç’°å¢ƒãƒã‚§ãƒƒã‚¯**")
with colB:
    st.info(tmsg if ok else tmsg)

with st.sidebar:
    st.header("OCR è¨­å®šï¼ˆç²¾åº¦èª¿æ•´ï¼‰")

    lang = st.text_input("è¨€èªï¼ˆTesseract langï¼‰", value=DEFAULT_LANG, help="ä¾‹: jpn / jpn+eng")
    dpi = st.slider("PDF â†’ ç”»åƒåŒ– DPI", 150, 400, DEFAULT_DPI, 10)
    psm = st.selectbox("PSMï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼‰", options=[3, 4, 6, 11, 12], index=2, help="6=ãƒ–ãƒ­ãƒƒã‚¯ã€11/12=ç–ãªãƒ†ã‚­ã‚¹ãƒˆã«å¼·ã‚")
    oem = st.selectbox("OEMï¼ˆã‚¨ãƒ³ã‚¸ãƒ³ï¼‰", options=[1, 3], index=1, help="3=æ—¢å®šï¼ˆLSTMå„ªå…ˆï¼‰")

    st.divider()
    st.subheader("å‰å‡¦ç†")
    scale = st.slider("æ‹¡å¤§å€ç‡", 1.0, 3.0, 1.6, 0.1)
    denoise = st.selectbox("ãƒã‚¤ã‚ºé™¤å»", options=[0, 1, 2], index=1, help="æ‰‹æ›¸ãã‚„ã‚¹ã‚­ãƒ£ãƒ³ã¯ 1ã€œ2 ãŒå®‰å®š")
    contrast = st.slider("ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ", -50, 80, 25, 5)
    brightness = st.slider("æ˜ã‚‹ã•", -50, 50, 0, 5)

    binarize = st.checkbox("äºŒå€¤åŒ–ã™ã‚‹", value=True)
    adaptive = st.checkbox("é©å¿œçš„äºŒå€¤åŒ–ï¼ˆæ¿ƒæ·¡ã«å¼·ã„ï¼‰", value=True)
    invert = st.checkbox("ç™½é»’åè»¢ï¼ˆç™½æ–‡å­—/é»’èƒŒæ™¯ãªã©ï¼‰", value=False)
    sharpen = st.checkbox("ã‚·ãƒ£ãƒ¼ãƒ—åŒ–", value=True)

    st.divider()
    st.header("æ¤œç´¢è¨­å®š")
    chunk_size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 400, 1800, 900, 50)
    overlap = st.slider("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", 0, 400, 150, 10)
    top_k = st.slider("ä¸Šä½è¡¨ç¤ºä»¶æ•°", 3, 20, 8, 1)
    min_score = st.slider("æœ€å°ã‚¹ã‚³ã‚¢ï¼ˆè¶³åˆ‡ã‚Šï¼‰", 0.0, 1.0, 0.10, 0.01)

st.divider()

tab1, tab2 = st.tabs(["â‘  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦OCR", "â‘¡ OCRæ¸ˆã¿JSONã‚’èª­ã¿è¾¼ã¿"])

# ã‚»ãƒƒã‚·ãƒ§ãƒ³
if "doc" not in st.session_state:
    st.session_state["doc"] = None
if "index" not in st.session_state:
    st.session_state["index"] = None
if "index_items" not in st.session_state:
    st.session_state["index_items"] = []

# ---- â‘  OCR ----
with tab1:
    st.subheader("PDF / ç”»åƒ ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦OCR")

    up = st.file_uploader(
        "PDFã¾ãŸã¯ç”»åƒï¼ˆpng/jpgï¼‰ã‚’é¸æŠ",
        type=["pdf", "png", "jpg", "jpeg"],
        accept_multiple_files=False,
    )

    run_btn = st.button("OCR å®Ÿè¡Œ", type="primary", disabled=(up is None))

    preview_col1, preview_col2 = st.columns([1, 1])

    if run_btn and up is not None:
        if not ok:
            st.error(
                "OCRã‚¨ãƒ©ãƒ¼: tesseract ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n"
                "Streamlit Cloud ã§ã¯ packages.txt ã§ tesseract-ocr ã¨ tesseract-ocr-jpn ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚"
            )
        else:
            with st.spinner("OCRä¸­...ï¼ˆãƒšãƒ¼ã‚¸æ•°ã‚„DPIã«ã‚ˆã‚Šæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
                fname = safe_filename(up.name)
                b = up.read()

                pages_text: List[str] = []
                preview_images: List[Image.Image] = []

                if fname.lower().endswith(".pdf"):
                    try:
                        images = pdf_to_images(b, dpi=dpi)
                    except Exception as e:
                        st.error(f"PDFã®ç”»åƒåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆPoppleræœªå°å…¥ã®å¯èƒ½æ€§ï¼‰: {e}")
                        st.stop()

                    for i, pil_img in enumerate(images, start=1):
                        img_bgr = pil_to_cv(pil_img)
                        pre = preprocess_for_ocr(
                            img_bgr,
                            scale=scale,
                            denoise=denoise,
                            contrast=contrast,
                            brightness=brightness,
                            binarize=binarize,
                            adaptive=adaptive,
                            invert=invert,
                            sharpen=sharpen,
                        )
                        text = run_tesseract(pre, lang=lang, psm=int(psm), oem=int(oem))
                        pages_text.append(text)
                        if i <= 2:
                            preview_images.append(cv_to_pil(pre))

                else:
                    pil_img = Image.open(io.BytesIO(b)).convert("RGB")
                    img_bgr = pil_to_cv(pil_img)
                    pre = preprocess_for_ocr(
                        img_bgr,
                        scale=scale,
                        denoise=denoise,
                        contrast=contrast,
                        brightness=brightness,
                        binarize=binarize,
                        adaptive=adaptive,
                        invert=invert,
                        sharpen=sharpen,
                    )
                    text = run_tesseract(pre, lang=lang, psm=int(psm), oem=int(oem))
                    pages_text = [text]
                    preview_images = [cv_to_pil(pre)]

                meta = {
                    "source": "upload",
                    "filename": fname,
                    "dpi": dpi,
                    "lang": lang,
                    "psm": int(psm),
                    "oem": int(oem),
                    "preprocess": {
                        "scale": scale,
                        "denoise": denoise,
                        "contrast": contrast,
                        "brightness": brightness,
                        "binarize": binarize,
                        "adaptive": adaptive,
                        "invert": invert,
                        "sharpen": sharpen,
                    },
                }

                doc = make_doc_json(doc_name=fname, pages_text=pages_text, meta=meta)
                st.session_state["doc"] = doc

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                items = doc_to_index_items(doc, chunk_size=chunk_size, overlap=overlap)
                idx = TfIdfSearchIndex()
                if items:
                    idx.build(items)
                st.session_state["index_items"] = items
                st.session_state["index"] = idx

            st.success("OCRå®Œäº†ï¼†æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

            with preview_col1:
                st.write("**å‰å‡¦ç†ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€å¤§2æšï¼‰**")
                for im in preview_images[:2]:
                    st.image(im, use_container_width=True)

            with preview_col2:
                st.write("**OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ˆé ­ãƒšãƒ¼ã‚¸ï¼‰**")
                st.text_area("",
                             value=(pages_text[0] if pages_text else ""),
                             height=260)

            # JSONä¿å­˜ / ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.write("### OCRçµæœï¼ˆJSONï¼‰")
            json_bytes = json.dumps(doc, ensure_ascii=False, indent=2).encode("utf-8")
            st.download_button(
                "JSONã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=json_bytes,
                file_name=f"{os.path.splitext(fname)[0]}.json",
                mime="application/json",
            )

# ---- â‘¡ JSONèª­ã¿è¾¼ã¿ ----
with tab2:
    st.subheader("OCRæ¸ˆã¿JSONï¼ˆocr_doc_v1ï¼‰ã‚’èª­ã¿è¾¼ã¿")
    jup = st.file_uploader("JSONã‚’é¸æŠ", type=["json"], accept_multiple_files=False, key="json_uploader")
    load_btn = st.button("JSON èª­ã¿è¾¼ã¿", disabled=(jup is None))

    if load_btn and jup is not None:
        try:
            doc = load_doc_json(jup.read())
            if doc.get("schema") != "ocr_doc_v1":
                st.warning("schema ãŒ ocr_doc_v1 ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆèª­ã¿è¾¼ã¿ã¯ç¶™ç¶šã—ã¾ã™ï¼‰ã€‚")
            st.session_state["doc"] = doc

            items = doc_to_index_items(doc, chunk_size=chunk_size, overlap=overlap)
            idx = TfIdfSearchIndex()
            if items:
                idx.build(items)
            st.session_state["index_items"] = items
            st.session_state["index"] = idx

            st.success("JSONã‚’èª­ã¿è¾¼ã¿ã€æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"JSONèª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")

st.divider()

# =========================
# æ¤œç´¢UI
# =========================
doc = st.session_state.get("doc")
idx: TfIdfSearchIndex = st.session_state.get("index")

if doc is None or idx is None or not st.session_state.get("index_items"):
    st.warning("ã¾ãšã€Œâ‘  OCRã€ã¾ãŸã¯ã€Œâ‘¡ JSONèª­ã¿è¾¼ã¿ã€ã§æ–‡æ›¸ã‚’æº–å‚™ã—ã¦ãã ã•ã„ã€‚")
else:
    left, right = st.columns([2, 1])
    with left:
        q = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ä¾‹: ææ–™ / ç”³è«‹ / å®šå“¡ / A5052 / 6061-T6 ...")
    with right:
        st.write(" ")
        clear = st.button("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢")
        if clear:
            st.session_state["doc"] = None
            st.session_state["index"] = None
            st.session_state["index_items"] = []
            st.rerun()

    if q.strip():
        results = idx.search(q, top_k=top_k)
        # è¶³åˆ‡ã‚Š
        results = [(it, sc) for it, sc in results if sc >= float(min_score)]

        st.write(f"**ãƒ’ãƒƒãƒˆä»¶æ•°ï¼ˆä¸Šä½è¡¨ç¤ºï¼‰:** {len(results)} ä»¶ï¼ˆmin_score={min_score}ï¼‰")

        if not results:
            st.info("è©²å½“ã™ã‚‹çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚PSMã‚’ 11/12 ã«å¤‰ãˆã‚‹ã€DPIã‚’ä¸Šã’ã‚‹ã€äºŒå€¤åŒ–/åè»¢ã‚’è©¦ã™ã®ãŒæœ‰åŠ¹ã§ã™ã€‚")
        else:
            for rank, (it, sc) in enumerate(results, start=1):
                title = f"#{rank}  {it.doc_name} / p.{it.page} / score={sc:.3f}"
                with st.expander(title, expanded=(rank <= 2)):
                    st.markdown(highlight_snippet(it.text, q, max_len=450))

            # å‚è€ƒï¼šãƒšãƒ¼ã‚¸åˆ¥ã®æ–‡å­—æ•°ã‚µãƒãƒª
            pages = doc.get("pages", [])
            df = pd.DataFrame([{"page": p.get("page"), "chars": len((p.get("text") or "").strip())} for p in pages])
            if not df.empty:
                st.caption("ãƒšãƒ¼ã‚¸åˆ¥ æ–‡å­—æ•°ï¼ˆOCRã§æ–‡å­—ãŒã»ã¼å–ã‚Œã¦ã„ãªã„ãƒšãƒ¼ã‚¸ã®è¦‹ã¤ã‘ã«æœ‰åŠ¹ï¼‰")
                st.dataframe(df, use_container_width=True, hide_index=True)

    else:
        st.info("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

st.caption("â€»å…±æœ‰ï¼ˆStreamlit Cloudï¼‰ã§ä½¿ã†å ´åˆï¼špackages.txt ã§ tesseract/poppler ã‚’Cloudå´ã«å°å…¥ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼PCã«ã¯ä¸è¦ã§ã™ã€‚")




