"""
ÊäÄË°ìË≥áÊñôRAGÊ§úÁ¥¢„Ç¢„Éó„É™ÔºàCloudÊ§úÁ¥¢Â∞ÇÁî®Áâà / „É´„Éº„ÉàCÔºâ
- „É≠„Éº„Ç´„É´„ÅßOCR‚ÜíJSONÂåñ„Åó„ÅüÁµêÊûúÔºàdata/ocr_results/*.jsonÔºâ„ÇíË™≠„ÅøËæº„Åø
- ChromaDB + SentenceTransformers „ÅßÊ§úÁ¥¢ÔºàRAG„ÅÆ„ÄåR„ÄçÈÉ®ÂàÜÔºâ
- Streamlit Cloud„Åß„ÅØOCR„Çí‰∏ÄÂàá„Åó„Å™„ÅÑÔºàTesseract‰∏çË¶ÅÔºâ
"""

from __future__ import annotations

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

import streamlit as st

# -----------------------------
# Ë®≠ÂÆöË™≠„ÅøËæº„ÅøÔºàconfig.py„Åå„ÅÇ„Çå„Å∞‰Ωø„ÅÜÔºâ
# -----------------------------
try:
    from config import (
        DATA_DIR,
        OCR_RESULTS_DIR,
        VECTOR_DB_DIR,
        VECTOR_DB_COLLECTION_NAME,
        EMBEDDING_MODEL_NAME,
        DEFAULT_SEARCH_RESULTS,
        MAX_SEARCH_RESULTS,
    )
except Exception:
    DATA_DIR = Path("data")
    OCR_RESULTS_DIR = DATA_DIR / "ocr_results"
    VECTOR_DB_DIR = DATA_DIR / "chroma_db"
    VECTOR_DB_COLLECTION_NAME = "technical_documents"
    EMBEDDING_MODEL_NAME = "paraphrase-multilingual-mpnet-base-v2"
    DEFAULT_SEARCH_RESULTS = 5
    MAX_SEARCH_RESULTS = 10

# „Éá„Ç£„É¨„ÇØ„Éà„É™Á¢∫‰øù
OCR_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)

# -----------------------------
# RAG„É©„Ç§„Éñ„É©„É™ÔºàÂøÖÈ†àÔºâ
# -----------------------------
try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    CHROMA_OK = True
except Exception:
    CHROMA_OK = False

# -----------------------------
# UI
# -----------------------------
st.set_page_config(
    page_title="ÊäÄË°ìË≥áÊñôRAGÊ§úÁ¥¢ÔºàCloudÊ§úÁ¥¢Â∞ÇÁî®Ôºâ",
    page_icon="üîé",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.title("üîé ÊäÄË°ìË≥áÊñôRAGÊ§úÁ¥¢ÔºàCloudÊ§úÁ¥¢Â∞ÇÁî® / „É´„Éº„ÉàCÔºâ")
st.caption("‚Äª „Åì„ÅÆCloud„Ç¢„Éó„É™„ÅØOCR„Åó„Åæ„Åõ„Çì„ÄÇ„É≠„Éº„Ç´„É´„Åß‰Ωú„Å£„ÅüJSON„ÇíË™≠„ÅøËæº„Çì„ÅßÊ§úÁ¥¢„Åó„Åæ„Åô„ÄÇ")
st.markdown("---")


# -----------------------------
# „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£
# -----------------------------
def load_json_documents(json_dir: Path) -> List[Dict[str, Any]]:
    """data/ocr_results/*.json „ÇíË™≠„ÅøËæº„ÇÄ"""
    docs: List[Dict[str, Any]] = []
    for p in sorted(json_dir.glob("*.json")):
        try:
            obj = json.loads(p.read_text(encoding="utf-8"))
            # ÂøÖÈ†à„Ç≠„Éº„ÇíÊ≠£Ë¶èÂåñ
            doc_id = obj.get("id") or p.stem
            filename = obj.get("filename") or obj.get("source") or p.name
            text = obj.get("text") or ""
            uploaded_at = obj.get("uploaded_at") or obj.get("created_at") or ""
            ocr_settings = obj.get("ocr_settings") or {}

            docs.append(
                {
                    "id": str(doc_id),
                    "filename": str(filename),
                    "text": str(text),
                    "uploaded_at": str(uploaded_at),
                    "ocr_settings": ocr_settings,
                    "_path": str(p),
                }
            )
        except Exception:
            # Â£ä„Çå„ÅüJSON„ÅåÊ∑∑„Åñ„Å£„Å¶„ÅÑ„Å¶„ÇÇËêΩ„Å®„Åï„Å™„ÅÑ
            continue
    return docs


@st.cache_resource
def get_embedding_model(model_name: str):
    return SentenceTransformer(model_name)


@st.cache_resource
def get_chroma_collection(persist_dir: str, collection_name: str):
    """
    Chroma „ÅÆÊ∞∏Á∂öDB„ÇíÈñã„ÅèÔºà„Éê„Éº„Ç∏„Éß„É≥Â∑ÆÂàÜ„Å´Âº∑„ÇÅ„Å´Ôºâ
    """
    # Êñ∞„Åó„ÇÅ„ÅÆAPI: PersistentClient
    try:
        client = chromadb.PersistentClient(path=persist_dir)
        try:
            col = client.get_collection(collection_name)
        except Exception:
            col = client.create_collection(collection_name)
        return col
    except Exception:
        # Âè§„ÅÑAPI: Client + Settings
        try:
            from chromadb.config import Settings

            client = chromadb.Client(
                Settings(chroma_db_impl="duckdb+parquet", persist_directory=persist_dir)
            )
            try:
                col = client.get_collection(collection_name)
            except Exception:
                col = client.create_collection(collection_name)
            return col
        except Exception as e:
            raise RuntimeError(f"ChromaÂàùÊúüÂåñ„Å´Â§±Êïó: {e}") from e


def ensure_indexed(
    docs: List[Dict[str, Any]],
    collection,
    model,
) -> Dict[str, Any]:
    """
    JSON„Éâ„Ç≠„É•„É°„É≥„Éà„ÇíChroma„Å´ÊäïÂÖ•ÔºàÊú™ÁôªÈå≤„ÅÆ„ÅøÔºâ
    """
    # Êó¢Â≠òID‰∏ÄË¶ß„ÇíÂèñÂæóÔºàÂ§ßÈáè„Å†„Å®Èáç„ÅÑ„ÅÆ„Åß„ÄÅ„Åæ„Åö„ÅØ docs „ÅÆid„Å†„ÅëÁ¢∫Ë™çÔºâ
    # Chroma„Å´ "get(ids=[...])" „ÅåÈÄö„Çå„Å∞„ÄÅ„Åù„Çå„Çí‰Ωø„ÅÜ
    to_add = []
    to_add_ids = []
    to_add_texts = []
    to_add_metas = []

    # doc„Åî„Å®„Å´Â≠òÂú®Á¢∫Ë™ç
    for d in docs:
        doc_id = d["id"]
        exists = False
        try:
            got = collection.get(ids=[doc_id])
            if got and got.get("ids") and len(got["ids"]) > 0:
                exists = True
        except Exception:
            # get(ids=) „ÅåÂ§±Êïó„Åô„ÇãÂÆüË£Ö„ÇÇ„ÅÇ„Çã„ÅÆ„Åß„ÄÅ„Åù„ÅÆÂ†¥Âêà„ÅØËøΩÂä†ÂÅ¥„ÅßÂºæ„Åã„Çå„ÇãÊÉ≥ÂÆö
            exists = False

        if not exists:
            text = (d.get("text") or "").strip()
            if not text:
                continue
            to_add.append(d)
            to_add_ids.append(doc_id)
            to_add_texts.append(text)
            to_add_metas.append(
                {
                    "filename": d.get("filename", ""),
                    "uploaded_at": d.get("uploaded_at", ""),
                    "json_path": d.get("_path", ""),
                }
            )

    if not to_add_ids:
        return {"added": 0}

    embeddings = model.encode(to_add_texts).tolist()
    collection.add(
        ids=to_add_ids,
        embeddings=embeddings,
        documents=to_add_texts,
        metadatas=to_add_metas,
    )
    return {"added": len(to_add_ids)}


def search(query: str, n_results: int, collection, model) -> List[Dict[str, Any]]:
    q_emb = model.encode([query]).tolist()
    res = collection.query(query_embeddings=q_emb, n_results=n_results)

    out: List[Dict[str, Any]] = []
    ids = (res.get("ids") or [[]])[0]
    docs = (res.get("documents") or [[]])[0]
    metas = (res.get("metadatas") or [[]])[0]
    dists = (res.get("distances") or [[]])[0]  # Â∞è„Åï„ÅÑ„Åª„Å©Ëøë„ÅÑ

    for i in range(len(ids)):
        out.append(
            {
                "id": ids[i],
                "text": docs[i],
                "metadata": metas[i] if i < len(metas) else {},
                "distance": dists[i] if i < len(dists) else None,
            }
        )
    return out


# -----------------------------
# Sidebar
# -----------------------------
with st.sidebar:
    st.header("‚öôÔ∏è CloudÊ§úÁ¥¢Â∞ÇÁî®Ôºà„É´„Éº„ÉàCÔºâ")
    st.write("‚úÖ OCR„ÅØ„É≠„Éº„Ç´„É´„ÅßÂÆüÊñΩ„Åó„ÄÅJSON„Çí„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„Å∏ÈÖçÁΩÆ„Åó„Åæ„Åô„ÄÇ")
    st.write(f"üìÅ JSONË™≠ËæºÂÖà: `{OCR_RESULTS_DIR.as_posix()}`")
    st.write(f"üß† Âüã„ÇÅËæº„Åø„É¢„Éá„É´: `{EMBEDDING_MODEL_NAME}`")
    st.write(f"üóÑÔ∏è ChromaDB: `{VECTOR_DB_DIR.as_posix()}`")

    st.markdown("---")

    if not CHROMA_OK:
        st.error("ChromaDB / sentence-transformers „Åårequirements„Å´ÂÖ•„Å£„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ")
        st.stop()

    if st.button("üîÑ JSON„ÇíÂÜçË™≠„ÅøËæº„ÅøÔºÜÂÜç„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ"):
        # cache„Çí‰Ωø„Å£„Å¶„ÅÑ„Å¶„ÇÇ docs „ÅØÊØéÂõûË™≠„ÇÄ„Åå„ÄÅ„É¶„Éº„Ç∂„Éº„Å´ÊòéÁ§∫„Åó„Åü„ÅÑ„ÅÆ„Åß rerun
        st.session_state["_force_reindex"] = True
        st.rerun()

    st.markdown("---")
    st.caption("‚Äª Streamlit Cloud„ÅÆ„Éï„Ç°„Ç§„É´„ÅØGitHub„Å´ÁΩÆ„ÅÑ„Åü„ÇÇ„ÅÆ„ÅåË™≠„Åæ„Çå„Åæ„Åô„ÄÇ")


# -----------------------------
# „É°„Ç§„É≥Âá¶ÁêÜ
# -----------------------------
docs = load_json_documents(OCR_RESULTS_DIR)

tab_search, tab_docs, tab_status = st.tabs(["üîç Ê§úÁ¥¢", "üìö JSON‰∏ÄË¶ß", "üß™ Áä∂ÊÖã/Ë®∫Êñ≠"])

with tab_status:
    st.subheader("Áä∂ÊÖã")
    st.write(f"JSON‰ª∂Êï∞: **{len(docs)}**")
    if len(docs) == 0:
        st.warning(
            "„Åæ„Å†JSON„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„É≠„Éº„Ç´„É´„Åß `local_ocr_to_json.py` „ÇíÂÆüË°å„Åó„Å¶JSON„Çí‰ΩúÊàê„Åó„ÄÅ"
            "`data/ocr_results/` „Å´ÂÖ•„Çå„Å¶GitHub„Å∏push„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        )

    st.write("Chroma/Model ÂàùÊúüÂåñ‚Ä¶")
    try:
        collection = get_chroma_collection(str(VECTOR_DB_DIR), VECTOR_DB_COLLECTION_NAME)
        model = get_embedding_model(EMBEDDING_MODEL_NAME)
        st.success("‚úÖ ChromaDB„Å®Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆÂàùÊúüÂåñOK")
    except Exception as e:
        st.error(f"ÂàùÊúüÂåñÂ§±Êïó: {e}")
        st.stop()

    # Ëá™Âãï„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ
    if len(docs) > 0:
        try:
            force = st.session_state.pop("_force_reindex", False)
            if force:
                # forceÊôÇ„ÅØÊó¢Â≠ò„ÇíÊ∂à„Åó„Åü„ÅÑ„Ç±„Éº„Çπ„ÇÇ„ÅÇ„Çã„Åå„ÄÅ„Åì„Åì„Åß„ÅØËøΩÂä†„ÅÆ„ÅøÔºàÂÆâÂÖ®Ôºâ
                st.info("ÂÜç„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÔºàËøΩÂä†Ôºâ„ÇíÂÆüË°å„Åó„Åæ„Åô‚Ä¶")
            result = ensure_indexed(docs, collection, model)
            st.write(f"‰ªäÂõûËøΩÂä†„Åó„Åü‰ª∂Êï∞: **{result.get('added', 0)}**")
        except Exception as e:
            st.error(f"„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÂ§±Êïó: {e}")

    st.markdown("---")
    st.write("üìå „É´„Éº„ÉàC„Åß„ÅØCloudÂÅ¥„Å´Tesseract„ÅØ‰∏çË¶Å„Åß„ÅôÔºàOCR„ÅØ„Åó„Åæ„Åõ„ÇìÔºâ„ÄÇ")


with tab_docs:
    st.subheader("JSON‰∏ÄË¶ßÔºàdata/ocr_resultsÔºâ")
    if len(docs) == 0:
        st.info("JSON„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Åæ„Åö„É≠„Éº„Ç´„É´„ÅßJSONÁîüÊàê‚ÜíGitHub„Å∏push„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
    else:
        for d in docs:
            with st.expander(f"üìÑ {d.get('filename','')}  |  {d.get('id','')}"):
                st.write(f"JSON: `{d.get('_path','')}`")
                if d.get("uploaded_at"):
                    st.write(f"Êó•ÊôÇ: {d.get('uploaded_at')}")
                st.write("„ÉÜ„Ç≠„Çπ„ÉàÔºàÂÖàÈ†≠500ÊñáÂ≠óÔºâ:")
                t = (d.get("text") or "")
                st.text(t[:500] + ("..." if len(t) > 500 else ""))


with tab_search:
    st.subheader("üîç Ê§úÁ¥¢ÔºàRAG„ÅÆRetrievalÔºâ")

    if not CHROMA_OK:
        st.stop()

    if len(docs) == 0:
        st.info("„Åæ„ÅöJSON„Çí `data/ocr_results/` „Å´ÂÖ•„Çå„Å¶GitHub„Å∏push„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
        st.stop()

    # ÂàùÊúüÂåñ
    collection = get_chroma_collection(str(VECTOR_DB_DIR), VECTOR_DB_COLLECTION_NAME)
    model = get_embedding_model(EMBEDDING_MODEL_NAME)

    # Ëá™Âãï„ÅßÊú™ÁôªÈå≤ÂàÜ„ÇíËøΩÂä†
    try:
        ensure_indexed(docs, collection, model)
    except Exception as e:
        st.error(f"„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÂá¶ÁêÜ„Åß„Ç®„É©„Éº: {e}")
        st.stop()

    query = st.text_input("Ê§úÁ¥¢„ÇØ„Ç®„É™", placeholder="‰æãÔºöÂ∑•Á®ãÁï∞Â∏∏„ÅÆÂéüÂõ†„ÄÅÊùêÊñô„ÅÆÁâπÊÄß„ÄÅË®≠ÂÇôÁÇπÊ§ú‚Ä¶")
    n_results = st.slider("Ê§úÁ¥¢ÁµêÊûúÊï∞", 1, int(MAX_SEARCH_RESULTS), int(DEFAULT_SEARCH_RESULTS))

    if st.button("üîé Ê§úÁ¥¢", type="primary") and query.strip():
        with st.spinner("Ê§úÁ¥¢‰∏≠‚Ä¶"):
            results = search(query.strip(), n_results, collection, model)

        if not results:
            st.info("Ê§úÁ¥¢ÁµêÊûú„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")
        else:
            st.success(f"Ê§úÁ¥¢ÁµêÊûú: {len(results)}‰ª∂")
            for i, r in enumerate(results, 1):
                dist = r.get("distance")
                score = None if dist is None else (1.0 / (1.0 + float(dist)))  # „Åñ„Å£„Åè„ÇäË°®Á§∫
                title = f"{i}. {r.get('id','')}"
                if score is not None:
                    title += f"  |  Ëøë„ÅïÁõÆÂÆâ: {score:.3f}"

                with st.expander(title):
                    st.write("„É°„Çø„Éá„Éº„Çø")
                    st.json(r.get("metadata") or {})
                    st.write("Êú¨ÊñáÔºàÂÖàÈ†≠800ÊñáÂ≠óÔºâ")
                    txt = r.get("text") or ""
                    st.text(txt[:800] + ("..." if len(txt) > 800 else ""))

