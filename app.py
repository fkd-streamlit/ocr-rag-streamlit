# app.py
# -----------------------------------------------
# OCR â†’ ãƒ†ã‚­ã‚¹ãƒˆåŒ– â†’ æ¤œç´¢ï¼ˆTF-IDFï¼‰ã¾ã§ã‚’ â€œStreamlit Cloud å˜ä½“â€ ã§å®Œçµã•ã›ã‚‹ç‰ˆ
# - ãƒ¦ãƒ¼ã‚¶ãƒ¼å´ã« Tesseract / Poppler ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸è¦ï¼ˆCloud å´ã§ packages.txt ã§å…¥ã‚Œã‚‹æƒ³å®šï¼‰
# - PDF / ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ OCR â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ æ¤œç´¢
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ OCR ç²¾åº¦ï¼ˆDPI/PSM/OEM/å‰å‡¦ç†ï¼‰ã‚’èª¿æ•´å¯èƒ½
# -----------------------------------------------

from __future__ import annotations

import io
import json
import math
import os
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

# ç”»åƒå‡¦ç†
import cv2
from PIL import Image

# OCR / PDF
import pytesseract
from pdf2image import convert_from_bytes

# æ¤œç´¢ï¼ˆTF-IDFï¼‰
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# =========================
# åŸºæœ¬è¨­å®š
# =========================
APP_TITLE = "OCR RAG Searchï¼ˆPDF/ç”»åƒ â†’ OCR â†’ æ¤œç´¢ï¼‰"
DEFAULT_LANG = "jpn+eng"  # jpnã®ã¿ã§ã‚‚å¯
DEFAULT_DPI = 250
DEFAULT_PSM = 6
DEFAULT_OEM = 3

# JSONï¼ˆOCRçµæœï¼‰ã®ä¿å­˜å…ˆï¼ˆCloudã§ã‚‚å‹•ãã‚ˆã†ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
DEFAULT_JSON_DIR = os.path.join("data", "ocr_results")


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def ensure_dirs() -> None:
    os.makedirs(DEFAULT_JSON_DIR, exist_ok=True)


def set_tesseract_cmd_if_needed() -> None:
    """
    Windowsãƒ­ãƒ¼ã‚«ãƒ«ç”¨ï¼šTESSERACT_CMD ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ã†ã€‚
    Streamlit Cloud ã§ã¯åŸºæœ¬ PATH ã« tesseract ãŒå…¥ã‚‹æƒ³å®šã€‚
    """
    cmd = os.environ.get("TESSERACT_CMD", "").strip()
    if cmd:
        pytesseract.pytesseract.tesseract_cmd = cmd


def is_tesseract_available() -> Tuple[bool, str]:
    """
    tesseract ãŒä½¿ãˆã‚‹ã‹è»½ããƒã‚§ãƒƒã‚¯
    """
    try:
        v = pytesseract.get_tesseract_version()
        return True, f"Tesseract: {v}"
    except Exception as e:
        return False, f"TesseractãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}"


def safe_filename(name: str) -> str:
    name = name.strip().replace("\\", "_").replace("/", "_")
    name = re.sub(r"[^\w\-\.\(\)ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]+", "_", name)
    return name[:120] if len(name) > 120 else name


def pil_to_cv(img: Image.Image) -> np.ndarray:
    arr = np.array(img.convert("RGB"))
    return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)


def cv_to_pil(img_bgr: np.ndarray) -> Image.Image:
    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    return Image.fromarray(rgb)


def preprocess_for_ocr(
    img_bgr: np.ndarray,
    *,
    scale: float = 1.5,
    denoise: int = 1,
    contrast: int = 20,   # -100..100
    brightness: int = 0,  # -100..100
    binarize: bool = True,
    adaptive: bool = True,
    invert: bool = False,
    sharpen: bool = True,
) -> np.ndarray:
    """
    æ‰‹æ›¸ã/æ¿ƒæ·¡/ã‚¹ã‚­ãƒ£ãƒ³ã®ãƒ–ãƒ¬ã«å¯¾å¿œã—ã‚„ã™ã„å‰å‡¦ç†ã‚»ãƒƒãƒˆ
    """
    h, w = img_bgr.shape[:2]
    if scale and scale != 1.0:
        img_bgr = cv2.resize(img_bgr, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_CUBIC)

    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

    # æ˜ã‚‹ã•ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
    # new = gray * (1 + contrast/100) + brightness
    alpha = 1.0 + (contrast / 100.0)
    beta = brightness
    gray = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)

    if denoise >= 1:
        # è»½ã„ãƒã‚¤ã‚ºé™¤å»
        gray = cv2.medianBlur(gray, 3)
    if denoise >= 2:
        # ã‚‚ã†å°‘ã—å¼·ã‚
        gray = cv2.bilateralFilter(gray, 7, 50, 50)

    if sharpen:
        kernel = np.array([[0, -1, 0],
                           [-1, 5, -1],
                           [0, -1, 0]], dtype=np.float32)
        gray = cv2.filter2D(gray, -1, kernel)

    if binarize:
        if adaptive:
            th = cv2.adaptiveThreshold(
                gray, 255,
                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                cv2.THRESH_BINARY,
                31, 10
            )
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        gray = th

    if invert:
        gray = cv2.bitwise_not(gray)

    # pytesseract ã¯ PIL ã‚‚å—ã‘ã‚‰ã‚Œã‚‹ãŒã€ã“ã“ã§ã¯ BGR ã«æˆ»ã™ï¼ˆè¡¨ç¤ºã«ã‚‚ä½¿ãˆã‚‹ï¼‰
    out_bgr = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
    return out_bgr


def run_tesseract(
    img_bgr: np.ndarray,
    *,
    lang: str,
    psm: int,
    oem: int,
) -> str:
    pil = cv_to_pil(img_bgr)
    config = f"--oem {oem} --psm {psm}"
    text = pytesseract.image_to_string(pil, lang=lang, config=config)
    return text.strip()


def pdf_to_images(pdf_bytes: bytes, dpi: int) -> List[Image.Image]:
    """
    Poppler ãŒå…¥ã£ã¦ã„ã‚Œã° convert_from_bytes ãŒå‹•ãï¼ˆStreamlit Cloud ã§ã¯ packages.txt ã§å°å…¥æƒ³å®šï¼‰
    """
    images = convert_from_bytes(pdf_bytes, dpi=dpi)
    return images


def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:
    text = re.sub(r"\s+\n", "\n", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    if not text:
        return []
    chunks = []
    i = 0
    n = len(text)
    while i < n:
        j = min(i + chunk_size, n)
        chunk = text[i:j].strip()
        if chunk:
            chunks.append(chunk)
        if j >= n:
            break
        i = max(0, j - overlap)
    return chunks


def highlight_snippet(s: str, q: str, max_len: int = 350) -> str:
    s2 = " ".join(s.split())
    q = q.strip()
    if not q:
        return (s2[:max_len] + "â€¦") if len(s2) > max_len else s2

    # ã‚¯ã‚¨ãƒªã¯ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®èªã‚‚æ‹¾ã†
    terms = [t for t in re.split(r"\s+", q) if t]
    # ã¾ãšæœ€åˆã®ä¸€è‡´ä½ç½®ã‚’æ¢ã™
    pos = None
    for t in terms:
        m = re.search(re.escape(t), s2, flags=re.IGNORECASE)
        if m:
            pos = m.start()
            break

    if pos is None:
        return (s2[:max_len] + "â€¦") if len(s2) > max_len else s2

    start = max(0, pos - max_len // 3)
    end = min(len(s2), start + max_len)
    snippet = s2[start:end]
    if start > 0:
        snippet = "â€¦" + snippet
    if end < len(s2):
        snippet = snippet + "â€¦"

    # å¼·èª¿ï¼ˆå¤ªå­—ï¼‰â€»markdown
    for t in sorted(terms, key=len, reverse=True):
        snippet = re.sub(
            re.escape(t),
            lambda m: f"**{m.group(0)}**",
            snippet,
            flags=re.IGNORECASE,
        )
    return snippet


# =========================
# TF-IDF ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
# =========================
@dataclass
class IndexedChunk:
    doc_name: str
    page: int
    chunk_id: int
    text: str


class TfIdfSearchIndex:
    def __init__(self) -> None:
        self.vectorizer: Optional[TfidfVectorizer] = None
        self.matrix = None
        self.items: List[IndexedChunk] = []

    def build(self, items: List[IndexedChunk]) -> None:
        self.items = items
        corpus = [it.text for it in items]
        self.vectorizer = TfidfVectorizer(
            lowercase=False,          # æ—¥æœ¬èªã®ãŸã‚
            token_pattern=r"(?u)\b\w+\b",
            ngram_range=(1, 2),
            max_features=80000,
        )
        self.matrix = self.vectorizer.fit_transform(corpus)

    def search(self, query: str, top_k: int = 8) -> List[Tuple[IndexedChunk, float]]:
        if not self.items or self.vectorizer is None or self.matrix is None:
            return []
        q = query.strip()
        if not q:
            return []
        qv = self.vectorizer.transform([q])
        sims = cosine_similarity(qv, self.matrix).flatten()
        if sims.size == 0:
            return []
        idxs = np.argsort(-sims)[:top_k]
        results = [(self.items[i], float(sims[i])) for i in idxs]
        return results


# =========================
# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå½¢å¼ï¼ˆJSONï¼‰
# =========================
def make_doc_json(
    doc_name: str,
    pages_text: List[str],
    meta: Dict[str, Any],
) -> Dict[str, Any]:
    return {
        "schema": "ocr_doc_v1",
        "doc_name": doc_name,
        "meta": meta,
        "pages": [{"page": i + 1, "text": pages_text[i]} for i in range(len(pages_text))],
    }


def load_doc_json(file_bytes: bytes) -> Dict[str, Any]:
    return json.loads(file_bytes.decode("utf-8"))


def doc_to_index_items(doc: Dict[str, Any], chunk_size: int, overlap: int) -> List[IndexedChunk]:
    doc_name = doc.get("doc_name", "document")
    items: List[IndexedChunk] = []
    for p in doc.get("pages", []):
        page_no = int(p.get("page", 0))
        text = p.get("text", "") or ""
        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)
        for ci, ch in enumerate(chunks):
            items.append(IndexedChunk(doc_name=doc_name, page=page_no, chunk_id=ci, text=ch))
    return items


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ§ ", layout="wide")
ensure_dirs()
set_tesseract_cmd_if_needed()

st.title(APP_TITLE)
st.caption("PDF/ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ OCR â†’ ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã€‚Cloudã§å…±æœ‰ã™ã‚‹å‰æã®æ§‹æˆã§ã™ã€‚")

ok, tmsg = is_tesseract_available()
colA, colB = st.columns([1, 2])
with colA:
    st.write("**ç’°å¢ƒãƒã‚§ãƒƒã‚¯**")
with colB:
    st.info(tmsg if ok else tmsg)

with st.sidebar:
    st.header("OCR è¨­å®šï¼ˆç²¾åº¦èª¿æ•´ï¼‰")

    lang = st.text_input("è¨€èªï¼ˆTesseract langï¼‰", value=DEFAULT_LANG, help="ä¾‹: jpn / jpn+eng")
    dpi = st.slider("PDF â†’ ç”»åƒåŒ– DPI", 150, 400, DEFAULT_DPI, 10)
    psm = st.selectbox("PSMï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼‰", options=[3, 4, 6, 11, 12], index=2, help="6=ãƒ–ãƒ­ãƒƒã‚¯ã€11/12=ç–ãªãƒ†ã‚­ã‚¹ãƒˆã«å¼·ã‚")
    oem = st.selectbox("OEMï¼ˆã‚¨ãƒ³ã‚¸ãƒ³ï¼‰", options=[1, 3], index=1, help="3=æ—¢å®šï¼ˆLSTMå„ªå…ˆï¼‰")

    st.divider()
    st.subheader("å‰å‡¦ç†")
    scale = st.slider("æ‹¡å¤§å€ç‡", 1.0, 3.0, 1.6, 0.1)
    denoise = st.selectbox("ãƒã‚¤ã‚ºé™¤å»", options=[0, 1, 2], index=1, help="æ‰‹æ›¸ãã‚„ã‚¹ã‚­ãƒ£ãƒ³ã¯ 1ã€œ2 ãŒå®‰å®š")
    contrast = st.slider("ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ", -50, 80, 25, 5)
    brightness = st.slider("æ˜ã‚‹ã•", -50, 50, 0, 5)

    binarize = st.checkbox("äºŒå€¤åŒ–ã™ã‚‹", value=True)
    adaptive = st.checkbox("é©å¿œçš„äºŒå€¤åŒ–ï¼ˆæ¿ƒæ·¡ã«å¼·ã„ï¼‰", value=True)
    invert = st.checkbox("ç™½é»’åè»¢ï¼ˆç™½æ–‡å­—/é»’èƒŒæ™¯ãªã©ï¼‰", value=False)
    sharpen = st.checkbox("ã‚·ãƒ£ãƒ¼ãƒ—åŒ–", value=True)

    st.divider()
    st.header("æ¤œç´¢è¨­å®š")
    chunk_size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 400, 1800, 900, 50)
    overlap = st.slider("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", 0, 400, 150, 10)
    top_k = st.slider("ä¸Šä½è¡¨ç¤ºä»¶æ•°", 3, 20, 8, 1)
    min_score = st.slider("æœ€å°ã‚¹ã‚³ã‚¢ï¼ˆè¶³åˆ‡ã‚Šï¼‰", 0.0, 1.0, 0.10, 0.01)

st.divider()

tab1, tab2 = st.tabs(["â‘  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦OCR", "â‘¡ OCRæ¸ˆã¿JSONã‚’èª­ã¿è¾¼ã¿"])

# ã‚»ãƒƒã‚·ãƒ§ãƒ³
if "doc" not in st.session_state:
    st.session_state["doc"] = None
if "index" not in st.session_state:
    st.session_state["index"] = None
if "index_items" not in st.session_state:
    st.session_state["index_items"] = []

# ---- â‘  OCR ----
with tab1:
    st.subheader("PDF / ç”»åƒ ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦OCR")

    up = st.file_uploader(
        "PDFã¾ãŸã¯ç”»åƒï¼ˆpng/jpgï¼‰ã‚’é¸æŠ",
        type=["pdf", "png", "jpg", "jpeg"],
        accept_multiple_files=False,
    )

    run_btn = st.button("OCR å®Ÿè¡Œ", type="primary", disabled=(up is None))

    preview_col1, preview_col2 = st.columns([1, 1])

    if run_btn and up is not None:
        if not ok:
            st.error(
                "OCRã‚¨ãƒ©ãƒ¼: tesseract ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n"
                "Streamlit Cloud ã§ã¯ packages.txt ã§ tesseract-ocr ã¨ tesseract-ocr-jpn ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚"
            )
        else:
            with st.spinner("OCRä¸­...ï¼ˆãƒšãƒ¼ã‚¸æ•°ã‚„DPIã«ã‚ˆã‚Šæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
                fname = safe_filename(up.name)
                b = up.read()

                pages_text: List[str] = []
                preview_images: List[Image.Image] = []

                if fname.lower().endswith(".pdf"):
                    try:
                        images = pdf_to_images(b, dpi=dpi)
                    except Exception as e:
                        st.error(f"PDFã®ç”»åƒåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆPoppleræœªå°å…¥ã®å¯èƒ½æ€§ï¼‰: {e}")
                        st.stop()

                    for i, pil_img in enumerate(images, start=1):
                        img_bgr = pil_to_cv(pil_img)
                        pre = preprocess_for_ocr(
                            img_bgr,
                            scale=scale,
                            denoise=denoise,
                            contrast=contrast,
                            brightness=brightness,
                            binarize=binarize,
                            adaptive=adaptive,
                            invert=invert,
                            sharpen=sharpen,
                        )
                        text = run_tesseract(pre, lang=lang, psm=int(psm), oem=int(oem))
                        pages_text.append(text)
                        if i <= 2:
                            preview_images.append(cv_to_pil(pre))

                else:
                    pil_img = Image.open(io.BytesIO(b)).convert("RGB")
                    img_bgr = pil_to_cv(pil_img)
                    pre = preprocess_for_ocr(
                        img_bgr,
                        scale=scale,
                        denoise=denoise,
                        contrast=contrast,
                        brightness=brightness,
                        binarize=binarize,
                        adaptive=adaptive,
                        invert=invert,
                        sharpen=sharpen,
                    )
                    text = run_tesseract(pre, lang=lang, psm=int(psm), oem=int(oem))
                    pages_text = [text]
                    preview_images = [cv_to_pil(pre)]

                meta = {
                    "source": "upload",
                    "filename": fname,
                    "dpi": dpi,
                    "lang": lang,
                    "psm": int(psm),
                    "oem": int(oem),
                    "preprocess": {
                        "scale": scale,
                        "denoise": denoise,
                        "contrast": contrast,
                        "brightness": brightness,
                        "binarize": binarize,
                        "adaptive": adaptive,
                        "invert": invert,
                        "sharpen": sharpen,
                    },
                }

                doc = make_doc_json(doc_name=fname, pages_text=pages_text, meta=meta)
                st.session_state["doc"] = doc

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                items = doc_to_index_items(doc, chunk_size=chunk_size, overlap=overlap)
                idx = TfIdfSearchIndex()
                if items:
                    idx.build(items)
                st.session_state["index_items"] = items
                st.session_state["index"] = idx

            st.success("OCRå®Œäº†ï¼†æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

            with preview_col1:
                st.write("**å‰å‡¦ç†ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€å¤§2æšï¼‰**")
                for im in preview_images[:2]:
                    st.image(im, use_container_width=True)

            with preview_col2:
                st.write("**OCRãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ˆé ­ãƒšãƒ¼ã‚¸ï¼‰**")
                st.text_area("",
                             value=(pages_text[0] if pages_text else ""),
                             height=260)

            # JSONä¿å­˜ / ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            st.write("### OCRçµæœï¼ˆJSONï¼‰")
            json_bytes = json.dumps(doc, ensure_ascii=False, indent=2).encode("utf-8")
            st.download_button(
                "JSONã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=json_bytes,
                file_name=f"{os.path.splitext(fname)[0]}.json",
                mime="application/json",
            )

# ---- â‘¡ JSONèª­ã¿è¾¼ã¿ ----
with tab2:
    st.subheader("OCRæ¸ˆã¿JSONï¼ˆocr_doc_v1ï¼‰ã‚’èª­ã¿è¾¼ã¿")
    jup = st.file_uploader("JSONã‚’é¸æŠ", type=["json"], accept_multiple_files=False, key="json_uploader")
    load_btn = st.button("JSON èª­ã¿è¾¼ã¿", disabled=(jup is None))

    if load_btn and jup is not None:
        try:
            doc = load_doc_json(jup.read())
            if doc.get("schema") != "ocr_doc_v1":
                st.warning("schema ãŒ ocr_doc_v1 ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆèª­ã¿è¾¼ã¿ã¯ç¶™ç¶šã—ã¾ã™ï¼‰ã€‚")
            st.session_state["doc"] = doc

            items = doc_to_index_items(doc, chunk_size=chunk_size, overlap=overlap)
            idx = TfIdfSearchIndex()
            if items:
                idx.build(items)
            st.session_state["index_items"] = items
            st.session_state["index"] = idx

            st.success("JSONã‚’èª­ã¿è¾¼ã¿ã€æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"JSONèª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")

st.divider()

# =========================
# æ¤œç´¢UI
# =========================
doc = st.session_state.get("doc")
idx: TfIdfSearchIndex = st.session_state.get("index")

if doc is None or idx is None or not st.session_state.get("index_items"):
    st.warning("ã¾ãšã€Œâ‘  OCRã€ã¾ãŸã¯ã€Œâ‘¡ JSONèª­ã¿è¾¼ã¿ã€ã§æ–‡æ›¸ã‚’æº–å‚™ã—ã¦ãã ã•ã„ã€‚")
else:
    left, right = st.columns([2, 1])
    with left:
        q = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ä¾‹: ææ–™ / ç”³è«‹ / å®šå“¡ / A5052 / 6061-T6 ...")
    with right:
        st.write(" ")
        clear = st.button("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢")
        if clear:
            st.session_state["doc"] = None
            st.session_state["index"] = None
            st.session_state["index_items"] = []
            st.rerun()

    if q.strip():
        results = idx.search(q, top_k=top_k)
        # è¶³åˆ‡ã‚Š
        results = [(it, sc) for it, sc in results if sc >= float(min_score)]

        st.write(f"**ãƒ’ãƒƒãƒˆä»¶æ•°ï¼ˆä¸Šä½è¡¨ç¤ºï¼‰:** {len(results)} ä»¶ï¼ˆmin_score={min_score}ï¼‰")

        if not results:
            st.info("è©²å½“ã™ã‚‹çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚PSMã‚’ 11/12 ã«å¤‰ãˆã‚‹ã€DPIã‚’ä¸Šã’ã‚‹ã€äºŒå€¤åŒ–/åè»¢ã‚’è©¦ã™ã®ãŒæœ‰åŠ¹ã§ã™ã€‚")
        else:
            for rank, (it, sc) in enumerate(results, start=1):
                title = f"#{rank}  {it.doc_name} / p.{it.page} / score={sc:.3f}"
                with st.expander(title, expanded=(rank <= 2)):
                    st.markdown(highlight_snippet(it.text, q, max_len=450))

            # å‚è€ƒï¼šãƒšãƒ¼ã‚¸åˆ¥ã®æ–‡å­—æ•°ã‚µãƒãƒª
            pages = doc.get("pages", [])
            df = pd.DataFrame([{"page": p.get("page"), "chars": len((p.get("text") or "").strip())} for p in pages])
            if not df.empty:
                st.caption("ãƒšãƒ¼ã‚¸åˆ¥ æ–‡å­—æ•°ï¼ˆOCRã§æ–‡å­—ãŒã»ã¼å–ã‚Œã¦ã„ãªã„ãƒšãƒ¼ã‚¸ã®è¦‹ã¤ã‘ã«æœ‰åŠ¹ï¼‰")
                st.dataframe(df, use_container_width=True, hide_index=True)

    else:
        st.info("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

st.caption("â€»å…±æœ‰ï¼ˆStreamlit Cloudï¼‰ã§ä½¿ã†å ´åˆï¼špackages.txt ã§ tesseract/poppler ã‚’Cloudå´ã«å°å…¥ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼PCã«ã¯ä¸è¦ã§ã™ã€‚")





# app.py  (Route A: ãƒ­ãƒ¼ã‚«ãƒ«ã§OCRâ†’JSONåŒ–â†’æ¤œç´¢) / Streamlit
# - data/ocr_results/*.json ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿ã—ã¦æ¤œç´¢
# - ã‚¯ã‚¨ãƒªã«å¿œã˜ã¦çµæœãŒå¤‰ã‚ã‚‹ï¼ˆãƒã‚°ä¿®æ­£ç‰ˆï¼‰
# - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(BM25é¢¨) + TF-IDF ã®ä¸¡å¯¾å¿œï¼ˆscikit-learnä½¿ç”¨ï¼‰
#
# æ³¨æ„: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯Streamlit Cloudã§å®Ÿè¡Œã•ã‚Œã¾ã™
# - pdf2imageã‚„pytesseractã¯ä½¿ç”¨ã—ã¾ã›ã‚“ï¼ˆOCRã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œï¼‰
# - å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª: streamlit, scikit-learn ã®ã¿

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Tuple

import streamlit as st


# ----------------------------
# è¨­å®š
# ----------------------------
OCR_RESULTS_DIR = Path("data") / "ocr_results"

DEFAULT_TOPK = 5

# å®¹é‡ä¸Šé™è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
# config.pyã‹ã‚‰èª­ã¿è¾¼ã‚‚ã†ã¨ã—ã¾ã™ãŒã€å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
MAX_JSON_FILE_SIZE_MB = 100
MAX_TOTAL_CHUNKS = 50000

# config.pyãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ä¸Šæ›¸ãï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import config
    MAX_JSON_FILE_SIZE_MB = getattr(config, 'MAX_JSON_FILE_SIZE_MB', MAX_JSON_FILE_SIZE_MB)
    MAX_TOTAL_CHUNKS = getattr(config, 'MAX_TOTAL_CHUNKS', MAX_TOTAL_CHUNKS)
except (ImportError, AttributeError, Exception):
    # config.pyãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    pass


# ----------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------
def normalize_text(s: str) -> str:
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def simple_tokenize_ja(s: str) -> List[str]:
    """
    å½¢æ…‹ç´ è§£æãªã—ã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã€‚
    - æ—¥æœ¬èª/è‹±æ•°å­—ã‚’ãã‚Œã£ã½ãåˆ†å‰²ã—ã¦ã€TF-IDFã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®å…¥åŠ›ã«ã™ã‚‹
    """
    s = normalize_text(s).lower()
    # ã²ã‚‰ãŒãª/ã‚«ã‚¿ã‚«ãƒŠ/æ¼¢å­—/è‹±æ•°å­—ã‚’ã¾ã¨ã‚ã¦æ‹¾ã†
    tokens = re.findall(r"[ä¸€-é¾¥]+|[ã-ã‚“]+|[ã‚¡-ãƒ´ãƒ¼]+|[a-z0-9]+", s)
    # 1æ–‡å­—ã ã‘ã¯ãƒã‚¤ã‚ºã«ãªã‚Šã‚„ã™ã„ã®ã§é™¤å¤–ï¼ˆå¿…è¦ãªã‚‰å¤–ã—ã¦ãã ã•ã„ï¼‰
    tokens = [t for t in tokens if len(t) >= 2]
    return tokens


def load_ocr_json(path: Path) -> List[Dict[str, Any]]:
    """
    local_ocr_to_json.py ãŒå‡ºåŠ›ã™ã‚‹æƒ³å®šã®JSON:
    { "meta":..., "pages":[{"page":1,"text":"..."}, ...] }
    ã‚‚ã—å½¢å¼ãŒé•ã£ã¦ã‚‚ã€pages/text ã‚’ã§ãã‚‹ã ã‘æ‹¾ã†
    """
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    file_size_mb = path.stat().st_size / (1024 * 1024)
    if file_size_mb > MAX_JSON_FILE_SIZE_MB:
        raise ValueError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™: {path.name} ({file_size_mb:.1f}MB > {MAX_JSON_FILE_SIZE_MB}MB)")
    
    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {path.name} - {e}")
    except MemoryError:
        raise MemoryError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¦ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {path.name} ({file_size_mb:.1f}MB)")

    pages = []
    if isinstance(data, dict) and "pages" in data and isinstance(data["pages"], list):
        for p in data["pages"]:
            txt = p.get("text", "")
            page_no = p.get("page", None)
            pages.append({"page": page_no, "text": normalize_text(txt)})
        return pages

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæƒ³å®šå¤–å½¢å¼ï¼‰
    if isinstance(data, list):
        for i, p in enumerate(data, start=1):
            if isinstance(p, dict) and "text" in p:
                pages.append({"page": p.get("page", i), "text": normalize_text(p.get("text", ""))})
    return pages


@dataclass
class Chunk:
    doc_id: str
    source_file: str
    page: int | None
    chunk_id: int
    text: str


def make_chunks(doc_id: str, source_file: str, pages: List[Dict[str, Any]], chunk_size: int = 900, overlap: int = 150) -> List[Chunk]:
    chunks: List[Chunk] = []
    cid = 0
    for p in pages:
        page_no = p.get("page", None)
        text = p.get("text", "")
        if not text:
            continue

        # ãƒšãƒ¼ã‚¸ã”ã¨ã«ã‚¹ãƒ©ã‚¤ãƒ‰ãƒãƒ£ãƒ³ã‚¯
        start = 0
        while start < len(text):
            end = min(len(text), start + chunk_size)
            ctext = text[start:end]
            chunks.append(
                Chunk(
                    doc_id=doc_id,
                    source_file=source_file,
                    page=page_no,
                    chunk_id=cid,
                    text=ctext,
                )
            )
            cid += 1
            if end == len(text):
                break
            start = max(0, end - overlap)
    return chunks


def load_all_chunks(ocr_dir: Path) -> List[Chunk]:
    chunks: List[Chunk] = []
    if not ocr_dir.exists():
        return chunks

    json_files = sorted(ocr_dir.glob("*.json"))
    if not json_files:
        return chunks
    
    skipped_files = []
    for jp in json_files:
        try:
            pages = load_ocr_json(jp)
            doc_id = jp.stem
            new_chunks = make_chunks(doc_id=doc_id, source_file=jp.name, pages=pages)
            
            # ç·ãƒãƒ£ãƒ³ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
            if len(chunks) + len(new_chunks) > MAX_TOTAL_CHUNKS:
                skipped_files.append(f"{jp.name} (ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šé™ã«é”ã—ã¾ã—ãŸ: {len(chunks) + len(new_chunks)} > {MAX_TOTAL_CHUNKS})")
                break
            
            chunks.extend(new_chunks)
        except (ValueError, MemoryError) as e:
            skipped_files.append(f"{jp.name} ({str(e)})")
            continue
    
    if skipped_files:
        import warnings
        warnings.warn(f"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ:\n" + "\n".join(f"  - {f}" for f in skipped_files))
    
    return chunks


# ----------------------------
# æ¤œç´¢ï¼ˆ1ï¼‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“BM25é¢¨ï¼‰
# ----------------------------
def keyword_score(query: str, text: str) -> float:
    q_tokens = simple_tokenize_ja(query)
    if not q_tokens:
        return 0.0
    t = normalize_text(text).lower()
    score = 0.0
    for tok in q_tokens:
        # å‡ºç¾å›æ•°ã‚’åŠ ç‚¹ï¼ˆè»½ã„é‡ã¿ï¼‰
        c = t.count(tok)
        if c > 0:
            score += 1.0 + min(3.0, c * 0.3)
    return score


# ----------------------------
# æ¤œç´¢ï¼ˆ2ï¼‰TF-IDF
# ----------------------------
@st.cache_resource(show_spinner=False)
def build_tfidf_index(texts: List[str]):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    vectorizer = TfidfVectorizer(
        tokenizer=simple_tokenize_ja,
        lowercase=True,
        min_df=1,
    )
    X = vectorizer.fit_transform(texts)

    def search(q: str) -> List[float]:
        qv = vectorizer.transform([q])
        sims = cosine_similarity(qv, X).flatten()
        return sims.tolist()

    return search


def search_chunks(chunks: List[Chunk], query: str, topk: int = 5) -> List[Tuple[float, Chunk]]:
    if not chunks:
        return []

    texts = [c.text for c in chunks]
    tfidf_search = build_tfidf_index(texts)
    tfidf_scores = tfidf_search(query)

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚æ··ãœã¦æœ€çµ‚ã‚¹ã‚³ã‚¢
    scored: List[Tuple[float, Chunk]] = []
    for s, c in zip(tfidf_scores, chunks):
        ks = keyword_score(query, c.text)
        final = float(s) * 1.0 + ks * 0.25  # â†æ··åˆæ¯”ç‡ï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰
        scored.append((final, c))

    scored.sort(key=lambda x: x[0], reverse=True)
    # ã‚¹ã‚³ã‚¢ãŒã»ã¼ã‚¼ãƒ­ã®ã‚‚ã®ã¯é™¤å¤–ï¼ˆãŸã ã—å…¨éƒ¨ã‚¼ãƒ­ãªã‚‰ãƒˆãƒƒãƒ—ã‚’è¿”ã™ï¼‰
    if scored and scored[0][0] <= 1e-8:
        return scored[:topk]
    return [x for x in scored[:topk] if x[0] > 1e-8] or scored[:topk]


# ----------------------------
# UI
# ----------------------------
st.set_page_config(page_title="OCR RAG (Local)", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ OCR RAGï¼ˆãƒ­ãƒ¼ã‚«ãƒ«OCRâ†’JSONâ†’æ¤œç´¢ï¼‰")

with st.sidebar:
    st.header("è¨­å®š")
    st.write("æ¤œç´¢å¯¾è±¡ï¼š `data/ocr_results/*.json`")
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
    st.subheader("ğŸ“¤ JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "OCRçµæœã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["json"],
        accept_multiple_files=True,
        help="local_ocr_to_json.pyã§ç”Ÿæˆã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    if uploaded_files:
        OCR_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        saved_count = 0
        for uploaded_file in uploaded_files:
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
                if file_size_mb > MAX_JSON_FILE_SIZE_MB:
                    st.warning(f"âš ï¸ {uploaded_file.name} ã¯ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ ({file_size_mb:.1f}MB > {MAX_JSON_FILE_SIZE_MB}MB)")
                    continue
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                save_path = OCR_RESULTS_DIR / uploaded_file.name
                with save_path.open("wb") as f:
                    f.write(uploaded_file.getvalue())
                saved_count += 1
                st.success(f"âœ… {uploaded_file.name} ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({file_size_mb:.1f}MB)")
            except Exception as e:
                st.error(f"âŒ {uploaded_file.name} ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        if saved_count > 0:
            st.info(f"â„¹ï¸ {saved_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ã€ŒJSONã‚’å†èª­ã¿è¾¼ã¿ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†èª­ã¿è¾¼ã¿ã‚’ä¿ƒã™
            if "chunks" in st.session_state:
                del st.session_state["chunks"]
    
    st.divider()
    
    topk = st.slider("è¡¨ç¤ºä»¶æ•° (Top-K)", 1, 10, DEFAULT_TOPK)
    chunk_size = st.number_input("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", min_value=300, max_value=3000, value=900, step=100)
    overlap = st.number_input("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", min_value=0, max_value=500, value=150, step=10)
    reload_btn = st.button("ğŸ”„ JSONã‚’å†èª­ã¿è¾¼ã¿")

# èª­ã¿è¾¼ã¿
if "chunks" not in st.session_state or reload_btn:
    with st.spinner("JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        try:
            raw_chunks = load_all_chunks(OCR_RESULTS_DIR)

            # ãƒãƒ£ãƒ³ã‚¯è¨­å®šå¤‰æ›´ã«å¯¾å¿œï¼ˆå†åˆ†å‰²ã—ãŸã„ï¼‰
            # ã„ã£ãŸã‚“ pages ã‚’èª­ã¿ç›´ã—ã¦ä½œã‚Šç›´ã™
            rebuilt: List[Chunk] = []
            skipped_count = 0
            
            json_files = sorted(OCR_RESULTS_DIR.glob("*.json"))
            progress_bar = st.progress(0)
            for idx, jp in enumerate(json_files):
                try:
                    pages = load_ocr_json(jp)
                    new_chunks = make_chunks(jp.stem, jp.name, pages, chunk_size=int(chunk_size), overlap=int(overlap))
                    
                    # ç·ãƒãƒ£ãƒ³ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
                    if len(rebuilt) + len(new_chunks) > MAX_TOTAL_CHUNKS:
                        st.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šé™ã«é”ã—ãŸãŸã‚ã€{jp.name} ä»¥é™ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                        skipped_count = len(json_files) - idx
                        break
                    
                    rebuilt.extend(new_chunks)
                    progress_bar.progress((idx + 1) / len(json_files))
                except (ValueError, MemoryError) as e:
                    st.warning(f"âš ï¸ {jp.name} ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {e}")
                    skipped_count += 1
                    continue
            
            progress_bar.empty()
            st.session_state["chunks"] = rebuilt
            
            if skipped_count > 0:
                st.info(f"â„¹ï¸ {skipped_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚å®¹é‡ä¸Šé™ã‚’èª¿æ•´ã™ã‚‹å ´åˆã¯ config.py ã‚’ç·¨é›†ã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.session_state["chunks"] = []

chunks: List[Chunk] = st.session_state["chunks"]

st.caption(f"èª­ã¿è¾¼ã¿JSONæ•°: {len(list(OCR_RESULTS_DIR.glob('*.json')))} / ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

if not chunks:
    st.warning("`data/ocr_results` ã« JSON ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.info("""
    **JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹æ–¹æ³•ï¼š**
    
    1. **ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰**
       - å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒğŸ“¤ JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    
    2. **ãƒ­ãƒ¼ã‚«ãƒ«ã§OCRã‚’å®Ÿè¡Œ**
       - `local_ocr_to_json.py` ã‚’ä½¿ã£ã¦PDF/ç”»åƒã‚’OCRã—ã€JSONã‚’ç”Ÿæˆ
       - ç”Ÿæˆã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ `data/ocr_results/` ã«é…ç½®
    
    3. **GitHubã«é…ç½®ï¼ˆStreamlit Cloudã®å ´åˆï¼‰**
       - JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ `data/ocr_results/` ã«é…ç½®ã—ã¦GitHubã«ãƒ—ãƒƒã‚·ãƒ¥
    """)
    st.stop()

query = st.text_input("æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šææ–™ / å®šå“¡ / 5052 / ã‚¢ãƒ«ãƒŸãƒ‹ã‚¦ãƒ ï¼‰", value="å®šå“¡")
go = st.button("ğŸ” æ¤œç´¢")

if go:
    results = search_chunks(chunks, query=query, topk=topk)

    st.subheader("æ¤œç´¢çµæœ")
    if not results:
        st.info("è©²å½“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¹ã‚³ã‚¢ãŒå…¨ã¦ã‚¼ãƒ­ï¼‰ã€‚åˆ¥ã®èªã‚„è¡¨è¨˜ã‚†ã‚Œã§è©¦ã—ã¦ãã ã•ã„ã€‚")
    else:
        for score, c in results:
            header = f"Score: {score:.4f} | File: {c.source_file} | Doc: {c.doc_id}"
            if c.page is not None:
                header += f" | Page: {c.page}"
            with st.expander(header, expanded=False):
                # ã‚¯ã‚¨ãƒªã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                t = c.text
                if query.strip():
                    t = re.sub(re.escape(query.strip()), lambda m: f"**{m.group(0)}**", t)
                st.write(t)


