# app.py
"""
æŠ€è¡“è³‡æ–™OCRãƒ»RAGæ¤œç´¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ«ãƒ¼ãƒˆCï¼šOCRã¯ãƒ­ãƒ¼ã‚«ãƒ«ã€Cloudã¯æ¤œç´¢å…±æœ‰ï¼‰
- ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½œæˆã—ãŸOCRçµæœJSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è“„ç©
- ChromaDB + SentenceTransformers ãŒã‚ã‚Œã°RAGæ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼‰
- ãªã„å ´åˆã§ã‚‚ç°¡æ˜“æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰ã§æœ€ä½é™å‹•ä½œ
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import streamlit as st

# -------------------------
# 0) è¨­å®šï¼ˆconfig.py ãŒã‚ã‚Œã°å„ªå…ˆï¼‰
# -------------------------
try:
    from config import (
        DATA_DIR,
        OCR_RESULTS_DIR,
        VECTOR_DB_DIR,
        VECTOR_DB_COLLECTION_NAME,
        EMBEDDING_MODEL_NAME,
        DEFAULT_SEARCH_RESULTS,
        MAX_SEARCH_RESULTS,
    )
except Exception:
    DATA_DIR = Path("data")
    OCR_RESULTS_DIR = DATA_DIR / "ocr_results"
    VECTOR_DB_DIR = DATA_DIR / "chroma_db"
    VECTOR_DB_COLLECTION_NAME = "technical_documents"
    EMBEDDING_MODEL_NAME = "paraphrase-multilingual-mpnet-base-v2"
    DEFAULT_SEARCH_RESULTS = 5
    MAX_SEARCH_RESULTS = 10

for d in [DATA_DIR, OCR_RESULTS_DIR, VECTOR_DB_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# -------------------------
# 1) RAGä¾å­˜ã®èª­ã¿è¾¼ã¿ï¼ˆã‚ã‚Œã°ä½¿ã†ï¼‰
# -------------------------
CHROMADB_AVAILABLE = True
CHROMA_IMPORT_ERROR = ""

try:
    import chromadb
    from chromadb.config import Settings
    from sentence_transformers import SentenceTransformer
except Exception as e:
    CHROMADB_AVAILABLE = False
    CHROMA_IMPORT_ERROR = str(e)

# -------------------------
# 2) UIè¨­å®š
# -------------------------
st.set_page_config(
    page_title="æŠ€è¡“è³‡æ–™OCRãƒ»RAGæ¤œç´¢ï¼ˆãƒ«ãƒ¼ãƒˆCï¼‰",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded",
)

# -------------------------
# 3) ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# -------------------------
REQUIRED_JSON_KEYS = {"id", "filename", "text", "uploaded_at"}

@dataclass
class Doc:
    id: str
    filename: str
    text: str
    uploaded_at: str
    meta: Dict[str, Any]

def now_id(prefix: str = "doc") -> str:
    return f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

def safe_load_json(b: bytes) -> Dict[str, Any]:
    return json.loads(b.decode("utf-8"))

def validate_doc_json(obj: Dict[str, Any]) -> Tuple[bool, str]:
    missing = REQUIRED_JSON_KEYS - set(obj.keys())
    if missing:
        return False, f"å¿…é ˆã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {sorted(list(missing))}"
    if not isinstance(obj.get("text", ""), str):
        return False, "text ã¯æ–‡å­—åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
    if not obj["id"]:
        return False, "id ãŒç©ºã§ã™"
    return True, ""

def normalize_doc(obj: Dict[str, Any]) -> Doc:
    meta = dict(obj)
    return Doc(
        id=str(obj["id"]),
        filename=str(obj.get("filename", "")),
        text=str(obj.get("text", "")),
        uploaded_at=str(obj.get("uploaded_at", "")),
        meta=meta,
    )

# -------------------------
# 4) JSONä¿ç®¡ï¼ˆCloudã§ã¯æ°¸ç¶šä¿è¨¼ãªã—ã ãŒã€å…±æœ‰ç”¨é€”ã¯ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€é‹ç”¨ã§OKï¼‰
# -------------------------
def list_saved_json_files() -> List[Path]:
    return sorted(OCR_RESULTS_DIR.glob("*.json"))

def load_docs_from_disk() -> List[Doc]:
    docs: List[Doc] = []
    for p in list_saved_json_files():
        try:
            obj = json.loads(p.read_text(encoding="utf-8"))
            ok, msg = validate_doc_json(obj)
            if not ok:
                continue
            docs.append(normalize_doc(obj))
        except Exception:
            continue
    return docs

def save_doc_to_disk(doc: Doc) -> Path:
    out = OCR_RESULTS_DIR / f"{doc.id}.json"
    with open(out, "w", encoding="utf-8") as f:
        json.dump(doc.meta, f, ensure_ascii=False, indent=2)
    return out

def delete_doc_files(doc_id: str) -> None:
    p = OCR_RESULTS_DIR / f"{doc_id}.json"
    if p.exists():
        p.unlink()

# -------------------------
# 5) ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ï¼ˆRAGç”¨ chunkingï¼‰
# -------------------------
def chunk_text(text: str, chunk_size: int = 800, overlap: int = 120) -> List[str]:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹åˆ†å‰²ï¼ˆæ—¥æœ¬èªå‘ã‘ã«å®‰å®šï¼‰
    """
    text = text.replace("\r\n", "\n")
    if len(text) <= chunk_size:
        return [text]

    chunks: List[str] = []
    i = 0
    n = len(text)
    while i < n:
        j = min(i + chunk_size, n)
        chunk = text[i:j].strip()
        if chunk:
            chunks.append(chunk)
        if j >= n:
            break
        i = max(0, j - overlap)
    return chunks

# -------------------------
# 6) RAGï¼ˆChromaDBï¼‰
# -------------------------
@st.cache_resource
def get_embedding_model() -> Optional["SentenceTransformer"]:
    if not CHROMADB_AVAILABLE:
        return None
    return SentenceTransformer(EMBEDDING_MODEL_NAME)

@st.cache_resource
def get_chroma_collection():
    if not CHROMADB_AVAILABLE:
        return None
    client = chromadb.Client(
        Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=str(VECTOR_DB_DIR),
        )
    )
    try:
        col = client.get_collection(VECTOR_DB_COLLECTION_NAME)
    except Exception:
        col = client.create_collection(VECTOR_DB_COLLECTION_NAME)
    return col

def rag_add_doc(doc: Doc) -> Tuple[bool, str]:
    """
    1æ–‡æ›¸ã‚’ chunk ã«åˆ†è§£ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ 
    """
    if not CHROMADB_AVAILABLE:
        return False, "ChromaDB/SentenceTransformers ãŒæœªå°å…¥ã§ã™"

    model = get_embedding_model()
    col = get_chroma_collection()
    if model is None or col is None:
        return False, "RAGåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ"

    chunks = chunk_text(doc.text)
    ids = [f"{doc.id}__c{i:04d}" for i in range(len(chunks))]
    metadatas = []
    for i in range(len(chunks)):
        metadatas.append(
            {
                "doc_id": doc.id,
                "chunk_index": i,
                "filename": doc.filename,
                "uploaded_at": doc.uploaded_at,
            }
        )

    try:
        emb = model.encode(chunks).tolist()
        col.add(ids=ids, embeddings=emb, documents=chunks, metadatas=metadatas)
        return True, f"RAGã«ç™»éŒ²ã—ã¾ã—ãŸï¼ˆchunks={len(chunks)}ï¼‰"
    except Exception as e:
        return False, f"RAGç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}"

def rag_delete_doc(doc_id: str) -> Tuple[bool, str]:
    if not CHROMADB_AVAILABLE:
        return False, "ChromaDBæœªå°å…¥"
    col = get_chroma_collection()
    if col is None:
        return False, "ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—å¤±æ•—"

    try:
        # doc_id__c0000 ã®ã‚ˆã†ãªIDã‚’ã¾ã¨ã‚ã¦å‰Šé™¤
        # where ã§ doc_id æŒ‡å®šã§ãã‚Œã°ãƒ™ã‚¹ãƒˆã ãŒã€ç’°å¢ƒå·®ãŒã‚ã‚‹ã®ã§ getâ†’filter ã§å¯¾å¿œ
        # å–å¾—ä»¶æ•°ãŒå¤šããªã‚‹å ´åˆã¯é‹ç”¨ã§åˆ†å‰²ã—ã¦ãã ã•ã„
        all_ids = col.get(include=["metadatas"]).get("ids", [])
        all_metas = col.get(include=["metadatas"]).get("metadatas", [])
        del_ids = []
        for _id, m in zip(all_ids, all_metas):
            if isinstance(m, dict) and m.get("doc_id") == doc_id:
                del_ids.append(_id)
        if del_ids:
            col.delete(ids=del_ids)
        return True, f"RAGã‹ã‚‰å‰Šé™¤ã—ã¾ã—ãŸï¼ˆ{len(del_ids)}ä»¶ï¼‰"
    except Exception as e:
        return False, f"RAGå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}"

def rag_search(query: str, n_results: int = 5) -> List[Dict[str, Any]]:
    if not CHROMADB_AVAILABLE:
        return []

    model = get_embedding_model()
    col = get_chroma_collection()
    if model is None or col is None:
        return []

    try:
        qemb = model.encode([query]).tolist()
        res = col.query(query_embeddings=qemb, n_results=n_results, include=["documents", "metadatas", "distances", "ids"])
        out: List[Dict[str, Any]] = []
        if res and res.get("ids") and len(res["ids"][0]) > 0:
            for i in range(len(res["ids"][0])):
                out.append(
                    {
                        "id": res["ids"][0][i],
                        "text": res["documents"][0][i],
                        "metadata": res["metadatas"][0][i],
                        "distance": res["distances"][0][i] if res.get("distances") else None,
                    }
                )
        return out
    except Exception:
        return []

# -------------------------
# 7) ç°¡æ˜“æ¤œç´¢ï¼ˆRAGãŒç„¡ã„ã¨ãï¼‰
# -------------------------
def simple_search(docs: List[Doc], query: str, limit: int = 5) -> List[Dict[str, Any]]:
    q = query.strip()
    if not q:
        return []
    hits = []
    for d in docs:
        idx = d.text.find(q)
        if idx >= 0:
            start = max(0, idx - 120)
            end = min(len(d.text), idx + 400)
            snippet = d.text[start:end]
            hits.append(
                {
                    "doc_id": d.id,
                    "filename": d.filename,
                    "snippet": snippet,
                    "pos": idx,
                }
            )
    hits.sort(key=lambda x: x["pos"])
    return hits[:limit]

# -------------------------
# 8) ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# -------------------------
if "documents" not in st.session_state:
    st.session_state.documents: List[Doc] = load_docs_from_disk()

# -------------------------
# 9) ã‚µã‚¤ãƒ‰ãƒãƒ¼
# -------------------------
with st.sidebar:
    st.header("âš™ï¸ å…±æœ‰ãƒ»æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ«ãƒ¼ãƒˆCï¼‰")

    st.markdown("**ã“ã®ã‚¢ãƒ—ãƒªã¯Cloudä¸Šã§OCRã—ã¾ã›ã‚“ã€‚** ä»£ã‚ã‚Šã«ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½œã£ãŸOCRçµæœJSONã‚’å–ã‚Šè¾¼ã¿ã¾ã™ã€‚")

    st.markdown("---")
    st.subheader("RAGçŠ¶æ…‹")
    if CHROMADB_AVAILABLE:
        st.success("âœ… RAGï¼ˆChromaDB + Embeddingï¼‰åˆ©ç”¨å¯èƒ½")
        st.caption(f"Embedding: {EMBEDDING_MODEL_NAME}")
    else:
        st.warning("âš ï¸ RAGã¯æœªä½¿ç”¨ï¼ˆç°¡æ˜“æ¤œç´¢ã§å‹•ä½œï¼‰")
        st.caption(f"ç†ç”±: {CHROMA_IMPORT_ERROR}")

    st.markdown("---")
    st.subheader("ä¿å­˜ãƒ‡ãƒ¼ã‚¿")
    st.write(f"JSONä¿å­˜å…ˆ: `{OCR_RESULTS_DIR.as_posix()}`")
    st.write(f"ä¿å­˜æ¸ˆã¿æ–‡æ›¸æ•°: **{len(st.session_state.documents)}**")

    if st.button("ğŸ”„ ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰å†èª­ã¿è¾¼ã¿"):
        st.session_state.documents = load_docs_from_disk()
        st.success("å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ")
        st.rerun()

# -------------------------
# 10) ãƒ¡ã‚¤ãƒ³UI
# -------------------------
st.title("ğŸ“„ æŠ€è¡“è³‡æ–™OCRãƒ»RAGæ¤œç´¢ï¼ˆãƒ«ãƒ¼ãƒˆCï¼šJSONå–ã‚Šè¾¼ã¿ â†’ å…±æœ‰æ¤œç´¢ï¼‰")
st.markdown("---")

tab_upload, tab_search, tab_list = st.tabs(["ğŸ“¤ JSONå–ã‚Šè¾¼ã¿", "ğŸ” æ¤œç´¢", "ğŸ“š æ–‡æ›¸ä¸€è¦§"])

# ========== Tab 1: JSONå–ã‚Šè¾¼ã¿ ==========
with tab_upload:
    st.header("ğŸ“¤ OCRçµæœJSONã®å–ã‚Šè¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã§ä½œæˆã—ãŸã‚‚ã®ï¼‰")

    st.markdown(
        """
- ãƒ­ãƒ¼ã‚«ãƒ«OCRã§ä½œã£ãŸ **JSONï¼ˆ1æ–‡æ›¸=1ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰** ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„  
- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã€ï¼ˆRAGãŒæœ‰åŠ¹ãªã‚‰ï¼‰ãƒ™ã‚¯ãƒˆãƒ«DBã«ã‚‚ç™»éŒ²ã§ãã¾ã™
"""
    )

    st.subheader("JSONã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    up = st.file_uploader("OCRçµæœJSONï¼ˆ.jsonï¼‰ã‚’é¸æŠ", type=["json"], accept_multiple_files=True)

    colA, colB = st.columns([1, 1])
    with colA:
        add_to_rag = st.checkbox("RAGã«ã‚‚ç™»éŒ²ã™ã‚‹ï¼ˆãŠã™ã™ã‚ï¼‰", value=CHROMADB_AVAILABLE, disabled=not CHROMADB_AVAILABLE)
    with colB:
        save_disk = st.checkbox("ã‚µãƒ¼ãƒå´ã«JSONã¨ã—ã¦ä¿å­˜ã™ã‚‹", value=True, help="Cloudã§ã¯æ°¸ç¶šä¿è¨¼ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆé‹ç”¨ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¨å¥¨ï¼‰")

    if up:
        for f in up:
            try:
                obj = safe_load_json(f.getvalue())
                ok, msg = validate_doc_json(obj)
                if not ok:
                    st.error(f"âŒ {f.name}: {msg}")
                    continue

                doc = normalize_doc(obj)

                # åŒIDãŒæ—¢ã«ã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¸Šæ›¸ãã—ãŸã„ãªã‚‰IDã‚’å¤‰ãˆã‚‹é‹ç”¨ï¼‰
                if any(d.id == doc.id for d in st.session_state.documents):
                    st.warning(f"âš ï¸ {doc.id} ã¯æ—¢ã«ç™»éŒ²æ¸ˆã¿ã§ã™ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                    continue

                st.session_state.documents.append(doc)

                if save_disk:
                    save_doc_to_disk(doc)

                if add_to_rag and CHROMADB_AVAILABLE:
                    ok2, msg2 = rag_add_doc(doc)
                    if ok2:
                        st.success(f"âœ… {doc.filename}: {msg2}")
                    else:
                        st.warning(f"âš ï¸ {doc.filename}: {msg2}")
                else:
                    st.success(f"âœ… {doc.filename}: å–ã‚Šè¾¼ã¿ã¾ã—ãŸ")

            except Exception as e:
                st.error(f"âŒ {f.name}: èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

        st.info("å–ã‚Šè¾¼ã¿å¾Œã¯ã€Œæ¤œç´¢ã€ã‚¿ãƒ–ã§æ¤œç´¢ã§ãã¾ã™ã€‚")

    st.markdown("---")
    st.subheader("JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹ï¼ˆå‚è€ƒï¼‰")
    example = {
        "id": "doc_20260106_120000",
        "filename": "æŠ€è¡“è³‡æ–™A.pdf",
        "text": "ï¼ˆOCRã§æŠ½å‡ºã—ãŸæœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆâ€¦ï¼‰",
        "uploaded_at": datetime.now().isoformat(),
        "ocr_settings": {
            "source": "local_ocr",
            "lang": "jpn",
            "psm": 6,
            "oem": 3
        }
    }
    st.code(json.dumps(example, ensure_ascii=False, indent=2), language="json")

# ========== Tab 2: æ¤œç´¢ ==========
with tab_search:
    st.header("ğŸ” æ¤œç´¢ï¼ˆRAGã¾ãŸã¯ç°¡æ˜“æ¤œç´¢ï¼‰")

    if len(st.session_state.documents) == 0:
        st.info("ğŸ“ ã¾ãšã€ŒJSONå–ã‚Šè¾¼ã¿ã€ã‚¿ãƒ–ã‹ã‚‰OCRçµæœJSONã‚’å–ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚")
    else:
        query = st.text_input("æ¤œç´¢ã‚¯ã‚¨ãƒª", placeholder="ä¾‹ï¼šé‡‘å‹æ¸©åº¦ã®è¨­å®šã€ææ–™ã®å¼·åº¦ã€ã‚µãƒ¼ãƒœèª¿æ•´â€¦")
        n_results = st.slider("æ¤œç´¢çµæœæ•°", 1, MAX_SEARCH_RESULTS, DEFAULT_SEARCH_RESULTS)

        if st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ", type="primary") and query.strip():
            if CHROMADB_AVAILABLE:
                with st.spinner("RAGæ¤œç´¢ä¸­..."):
                    results = rag_search(query, n_results=n_results)

                if results:
                    st.subheader(f"æ¤œç´¢çµæœï¼ˆRAGï¼‰: {len(results)}ä»¶")
                    for i, r in enumerate(results, 1):
                        meta = r.get("metadata") or {}
                        dist = r.get("distance")
                        score = None
                        if isinstance(dist, (int, float)):
                            # Chromaã¯è·é›¢ãŒå°ã•ã„ã»ã©è¿‘ã„ã€‚è¦‹ãŸç›®ç”¨ã«ã‚¹ã‚³ã‚¢åŒ–
                            score = 1.0 / (1.0 + dist)

                        title = f"çµæœ {i}: {meta.get('filename','(unknown)')} / doc={meta.get('doc_id','')}"
                        if score is not None:
                            title += f" / scoreâ‰ˆ{score:.3f}"

                        with st.expander(title):
                            st.write("**ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**")
                            st.json(meta)
                            st.write("**è©²å½“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆchunkï¼‰**")
                            st.text(r.get("text", ""))
                else:
                    st.info("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆRAGï¼‰")
                    st.caption("â€» å–ã‚Šè¾¼ã¿ç›´å¾Œã¯ã€RAGç™»éŒ²ã«å¤±æ•—ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚JSONå–ã‚Šè¾¼ã¿ã‚¿ãƒ–ã§ã€ŒRAGã«ã‚‚ç™»éŒ²ã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦å†å–ã‚Šè¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("ç°¡æ˜“æ¤œç´¢ä¸­..."):
                    hits = simple_search(st.session_state.documents, query, limit=n_results)

                if hits:
                    st.subheader(f"æ¤œç´¢çµæœï¼ˆç°¡æ˜“ï¼‰: {len(hits)}ä»¶")
                    for i, h in enumerate(hits, 1):
                        with st.expander(f"çµæœ {i}: {h['filename']} / doc={h['doc_id']}"):
                            st.text(h["snippet"])
                else:
                    st.info("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆç°¡æ˜“ï¼‰")

# ========== Tab 3: æ–‡æ›¸ä¸€è¦§ ==========
with tab_list:
    st.header("ğŸ“š ä¿å­˜æ¸ˆã¿æ–‡æ›¸ä¸€è¦§")

    if len(st.session_state.documents) == 0:
        st.info("ğŸ“ æ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚JSONå–ã‚Šè¾¼ã¿ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
    else:
        st.write(f"**æ–‡æ›¸æ•°: {len(st.session_state.documents)}**")

        for doc in st.session_state.documents:
            with st.expander(f"ğŸ“„ {doc.filename}ï¼ˆ{doc.id}ï¼‰"):
                st.write(f"**uploaded_at:** {doc.uploaded_at}")
                st.write(f"**æ–‡å­—æ•°:** {len(doc.text)}")

                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                preview = doc.text[:800] + ("..." if len(doc.text) > 800 else "")
                st.text(preview)

                c1, c2, c3 = st.columns([1, 1, 2])

                with c1:
                    # JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    st.download_button(
                        "â¬‡ï¸ JSONã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=json.dumps(doc.meta, ensure_ascii=False, indent=2).encode("utf-8"),
                        file_name=f"{doc.id}.json",
                        mime="application/json",
                        key=f"dl_{doc.id}",
                    )

                with c2:
                    if CHROMADB_AVAILABLE:
                        if st.button("ğŸ§  RAGã«ç™»éŒ²", key=f"rag_add_{doc.id}"):
                            ok, msg = rag_add_doc(doc)
                            (st.success if ok else st.warning)(msg)

                with c3:
                    if st.button("ğŸ—‘ï¸ å‰Šé™¤ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜åˆ†ã‚‚ï¼‰", key=f"del_{doc.id}"):
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å‰Šé™¤
                        st.session_state.documents = [d for d in st.session_state.documents if d.id != doc.id]

                        # ãƒ‡ã‚£ã‚¹ã‚¯JSONå‰Šé™¤
                        delete_doc_files(doc.id)

                        # RAGå‰Šé™¤
                        if CHROMADB_AVAILABLE:
                            rag_delete_doc(doc.id)

                        st.success("å‰Šé™¤ã—ã¾ã—ãŸ")
                        st.rerun()



