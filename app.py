# app.py  (Route A: ãƒ­ãƒ¼ã‚«ãƒ«ã§OCRâ†’JSONåŒ–â†’æ¤œç´¢) / Streamlit
# - data/ocr_results/*.json ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿ã—ã¦æ¤œç´¢
# - ã‚¯ã‚¨ãƒªã«å¿œã˜ã¦çµæœãŒå¤‰ã‚ã‚‹ï¼ˆãƒã‚°ä¿®æ­£ç‰ˆï¼‰
# - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰(BM25é¢¨) + TF-IDF ã®ä¸¡å¯¾å¿œï¼ˆscikit-learnä½¿ç”¨ï¼‰
#
# æ³¨æ„: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯Streamlit Cloudã§å®Ÿè¡Œã•ã‚Œã¾ã™
# - pdf2imageã‚„pytesseractã¯ä½¿ç”¨ã—ã¾ã›ã‚“ï¼ˆOCRã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œï¼‰
# - å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª: streamlit, scikit-learn ã®ã¿

from __future__ import annotations

import json
import re
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st

# OCRé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from PIL import Image, ImageOps, ImageEnhance, ImageFilter
    import pytesseract
    PIL_OK = True
except ImportError:
    PIL_OK = False

try:
    from pdf2image import convert_from_path, pdfinfo_from_path
    PDF2IMAGE_OK = True
except ImportError:
    PDF2IMAGE_OK = False


# ----------------------------
# è¨­å®š
# ----------------------------
OCR_RESULTS_DIR = Path("data") / "ocr_results"

DEFAULT_TOPK = 5

# å®¹é‡ä¸Šé™è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
# config.pyã‹ã‚‰èª­ã¿è¾¼ã‚‚ã†ã¨ã—ã¾ã™ãŒã€å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
MAX_JSON_FILE_SIZE_MB = 100
MAX_TOTAL_CHUNKS = 50000
MAX_PDF_FILE_SIZE_MB = 500
MAX_PDF_PAGES = 1000

# config.pyãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ä¸Šæ›¸ãï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import config
    MAX_JSON_FILE_SIZE_MB = getattr(config, 'MAX_JSON_FILE_SIZE_MB', MAX_JSON_FILE_SIZE_MB)
    MAX_TOTAL_CHUNKS = getattr(config, 'MAX_TOTAL_CHUNKS', MAX_TOTAL_CHUNKS)
    MAX_PDF_FILE_SIZE_MB = getattr(config, 'MAX_PDF_FILE_SIZE_MB', MAX_PDF_FILE_SIZE_MB)
    MAX_PDF_PAGES = getattr(config, 'MAX_PDF_PAGES', MAX_PDF_PAGES)
except (ImportError, AttributeError, Exception):
    # config.pyãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    pass


# ----------------------------
# OCRæ©Ÿèƒ½ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ----------------------------
def check_tesseract_available() -> Tuple[bool, str]:
    """TesseractãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
    if not PIL_OK:
        return False, "PIL/PillowãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    try:
        pytesseract.get_tesseract_version()
        return True, "Tesseractåˆ©ç”¨å¯èƒ½"
    except Exception as e:
        return False, f"TesseractãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}"

def preprocess_pil(img: Image.Image, contrast: float = 1.3, sharpen: bool = True) -> Image.Image:
    """ç”»åƒå‰å‡¦ç†"""
    if not PIL_OK:
        raise ImportError("PIL/PillowãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    x = img.convert("RGB")
    x = ImageOps.grayscale(x)
    x = ImageOps.autocontrast(x)
    if contrast and contrast != 1.0:
        x = ImageEnhance.Contrast(x).enhance(contrast)
    if sharpen:
        x = x.filter(ImageFilter.SHARPEN)
    return x

def ocr_image(img: Image.Image, lang: str = "jpn+eng", psm: int = 6, oem: int = 3) -> str:
    """ç”»åƒã‹ã‚‰OCRå®Ÿè¡Œ"""
    if not PIL_OK:
        raise ImportError("PIL/PillowãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    config_str = f"--oem {oem} --psm {psm} -l {lang}"
    return pytesseract.image_to_string(img, config=config_str)

def check_pdf_limits(pdf_bytes: bytes) -> Tuple[bool, str]:
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å®¹é‡ã‚’ãƒã‚§ãƒƒã‚¯"""
    file_size_mb = len(pdf_bytes) / (1024 * 1024)
    if file_size_mb > MAX_PDF_FILE_SIZE_MB:
        return False, f"PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™: {file_size_mb:.1f}MB > {MAX_PDF_FILE_SIZE_MB}MB"
    
    if not PDF2IMAGE_OK:
        return False, "pdf2imageãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    try:
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
            tmp.write(pdf_bytes)
            tmp_path = Path(tmp.name)
        
        try:
            info = pdfinfo_from_path(str(tmp_path))
            total_pages = int(info.get("Pages", 0))
            if total_pages > MAX_PDF_PAGES:
                return False, f"PDFãƒšãƒ¼ã‚¸æ•°ãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™: {total_pages}ãƒšãƒ¼ã‚¸ > {MAX_PDF_PAGES}ãƒšãƒ¼ã‚¸"
            return True, f"OK: {file_size_mb:.1f}MB, {total_pages}ãƒšãƒ¼ã‚¸"
        finally:
            tmp_path.unlink()
    except Exception as e:
        return False, f"PDFæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"

def process_pdf_upload(pdf_bytes: bytes, filename: str, dpi: int = 200, lang: str = "jpn+eng", 
                       psm: int = 6, oem: int = 3, progress_callback=None) -> Dict[str, Any]:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFã‚’OCRå‡¦ç†"""
    if not PDF2IMAGE_OK:
        raise RuntimeError("pdf2imageãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install pdf2image")
    if not PIL_OK:
        raise RuntimeError("PIL/PillowãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    import tempfile
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
        tmp.write(pdf_bytes)
        tmp_path = Path(tmp.name)
    
    try:
        started = time.time()
        pages = []
        
        # PDFæƒ…å ±å–å¾—
        info = pdfinfo_from_path(str(tmp_path))
        total_pages = int(info.get("Pages", 0))
        
        # 1ãƒšãƒ¼ã‚¸ãšã¤å‡¦ç†
        for p_no in range(1, total_pages + 1):
            images = convert_from_path(str(tmp_path), dpi=dpi, first_page=p_no, last_page=p_no)
            for img in images:
                proc = preprocess_pil(img)
                text = ocr_image(proc, lang=lang, psm=psm, oem=oem)
                pages.append({
                    "page": p_no,
                    "text": text,
                    "metadata": {"dpi": dpi, "lang": lang, "preprocess": ["grayscale", "autocontrast", "sharpen"]}
                })
                
                if progress_callback:
                    progress_callback(p_no, total_pages)
        
        return {
            "doc_id": Path(filename).stem,
            "title": filename,
            "source": filename,
            "created_at": datetime.utcnow().isoformat() + "Z",
            "pages": pages,
            "elapsed_sec": round(time.time() - started, 3)
        }
    finally:
        tmp_path.unlink()

def process_image_upload(img_bytes: bytes, filename: str, lang: str = "jpn+eng", 
                        psm: int = 6, oem: int = 3) -> Dict[str, Any]:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’OCRå‡¦ç†"""
    if not PIL_OK:
        raise RuntimeError("PIL/PillowãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    import io
    img = Image.open(io.BytesIO(img_bytes))
    proc = preprocess_pil(img)
    text = ocr_image(proc, lang=lang, psm=psm, oem=oem)
    
    return {
        "doc_id": Path(filename).stem,
        "title": filename,
        "source": filename,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "pages": [{
            "page": 1,
            "text": text,
            "metadata": {"dpi": None, "lang": lang, "preprocess": ["grayscale", "autocontrast", "sharpen"]}
        }]
    }


# ----------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------
def normalize_text(s: str) -> str:
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def simple_tokenize_ja(s: str) -> List[str]:
    """
    å½¢æ…‹ç´ è§£æãªã—ã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã€‚
    - æ—¥æœ¬èª/è‹±æ•°å­—ã‚’ãã‚Œã£ã½ãåˆ†å‰²ã—ã¦ã€TF-IDFã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®å…¥åŠ›ã«ã™ã‚‹
    """
    s = normalize_text(s).lower()
    # ã²ã‚‰ãŒãª/ã‚«ã‚¿ã‚«ãƒŠ/æ¼¢å­—/è‹±æ•°å­—ã‚’ã¾ã¨ã‚ã¦æ‹¾ã†
    tokens = re.findall(r"[ä¸€-é¾¥]+|[ã-ã‚“]+|[ã‚¡-ãƒ´ãƒ¼]+|[a-z0-9]+", s)
    # 1æ–‡å­—ã ã‘ã¯ãƒã‚¤ã‚ºã«ãªã‚Šã‚„ã™ã„ã®ã§é™¤å¤–ï¼ˆå¿…è¦ãªã‚‰å¤–ã—ã¦ãã ã•ã„ï¼‰
    tokens = [t for t in tokens if len(t) >= 2]
    return tokens


def load_ocr_json(path: Path) -> List[Dict[str, Any]]:
    """
    local_ocr_to_json.py ãŒå‡ºåŠ›ã™ã‚‹æƒ³å®šã®JSON:
    { "meta":..., "pages":[{"page":1,"text":"..."}, ...] }
    ã‚‚ã—å½¢å¼ãŒé•ã£ã¦ã‚‚ã€pages/text ã‚’ã§ãã‚‹ã ã‘æ‹¾ã†
    """
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    file_size_mb = path.stat().st_size / (1024 * 1024)
    if file_size_mb > MAX_JSON_FILE_SIZE_MB:
        raise ValueError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™: {path.name} ({file_size_mb:.1f}MB > {MAX_JSON_FILE_SIZE_MB}MB)")
    
    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {path.name} - {e}")
    except MemoryError:
        raise MemoryError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¦ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {path.name} ({file_size_mb:.1f}MB)")

    pages = []
    if isinstance(data, dict) and "pages" in data and isinstance(data["pages"], list):
        for p in data["pages"]:
            txt = p.get("text", "")
            page_no = p.get("page", None)
            pages.append({"page": page_no, "text": normalize_text(txt)})
        return pages

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæƒ³å®šå¤–å½¢å¼ï¼‰
    if isinstance(data, list):
        for i, p in enumerate(data, start=1):
            if isinstance(p, dict) and "text" in p:
                pages.append({"page": p.get("page", i), "text": normalize_text(p.get("text", ""))})
    return pages


@dataclass
class Chunk:
    doc_id: str
    source_file: str
    page: int | None
    chunk_id: int
    text: str


def make_chunks(doc_id: str, source_file: str, pages: List[Dict[str, Any]], chunk_size: int = 900, overlap: int = 150) -> List[Chunk]:
    chunks: List[Chunk] = []
    cid = 0
    for p in pages:
        page_no = p.get("page", None)
        text = p.get("text", "")
        if not text:
            continue

        # ãƒšãƒ¼ã‚¸ã”ã¨ã«ã‚¹ãƒ©ã‚¤ãƒ‰ãƒãƒ£ãƒ³ã‚¯
        start = 0
        while start < len(text):
            end = min(len(text), start + chunk_size)
            ctext = text[start:end]
            chunks.append(
                Chunk(
                    doc_id=doc_id,
                    source_file=source_file,
                    page=page_no,
                    chunk_id=cid,
                    text=ctext,
                )
            )
            cid += 1
            if end == len(text):
                break
            start = max(0, end - overlap)
    return chunks


def load_all_chunks(ocr_dir: Path) -> List[Chunk]:
    chunks: List[Chunk] = []
    if not ocr_dir.exists():
        return chunks

    json_files = sorted(ocr_dir.glob("*.json"))
    if not json_files:
        return chunks
    
    skipped_files = []
    for jp in json_files:
        try:
            pages = load_ocr_json(jp)
            doc_id = jp.stem
            new_chunks = make_chunks(doc_id=doc_id, source_file=jp.name, pages=pages)
            
            # ç·ãƒãƒ£ãƒ³ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
            if len(chunks) + len(new_chunks) > MAX_TOTAL_CHUNKS:
                skipped_files.append(f"{jp.name} (ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šé™ã«é”ã—ã¾ã—ãŸ: {len(chunks) + len(new_chunks)} > {MAX_TOTAL_CHUNKS})")
                break
            
            chunks.extend(new_chunks)
        except (ValueError, MemoryError) as e:
            skipped_files.append(f"{jp.name} ({str(e)})")
            continue
    
    if skipped_files:
        import warnings
        warnings.warn(f"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ:\n" + "\n".join(f"  - {f}" for f in skipped_files))
    
    return chunks


# ----------------------------
# æ¤œç´¢ï¼ˆ1ï¼‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“BM25é¢¨ï¼‰
# ----------------------------
def keyword_score(query: str, text: str) -> float:
    q_tokens = simple_tokenize_ja(query)
    if not q_tokens:
        return 0.0
    t = normalize_text(text).lower()
    score = 0.0
    for tok in q_tokens:
        # å‡ºç¾å›æ•°ã‚’åŠ ç‚¹ï¼ˆè»½ã„é‡ã¿ï¼‰
        c = t.count(tok)
        if c > 0:
            score += 1.0 + min(3.0, c * 0.3)
    return score


# ----------------------------
# æ¤œç´¢ï¼ˆ2ï¼‰TF-IDF
# ----------------------------
@st.cache_resource(show_spinner=False)
def build_tfidf_index(texts: List[str]):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    vectorizer = TfidfVectorizer(
        tokenizer=simple_tokenize_ja,
        lowercase=True,
        min_df=1,
    )
    X = vectorizer.fit_transform(texts)

    def search(q: str) -> List[float]:
        qv = vectorizer.transform([q])
        sims = cosine_similarity(qv, X).flatten()
        return sims.tolist()

    return search


def search_chunks(chunks: List[Chunk], query: str, topk: int = 5) -> List[Tuple[float, Chunk]]:
    if not chunks:
        return []

    texts = [c.text for c in chunks]
    tfidf_search = build_tfidf_index(texts)
    tfidf_scores = tfidf_search(query)

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚æ··ãœã¦æœ€çµ‚ã‚¹ã‚³ã‚¢
    scored: List[Tuple[float, Chunk]] = []
    for s, c in zip(tfidf_scores, chunks):
        ks = keyword_score(query, c.text)
        final = float(s) * 1.0 + ks * 0.25  # â†æ··åˆæ¯”ç‡ï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰
        scored.append((final, c))

    scored.sort(key=lambda x: x[0], reverse=True)
    # ã‚¹ã‚³ã‚¢ãŒã»ã¼ã‚¼ãƒ­ã®ã‚‚ã®ã¯é™¤å¤–ï¼ˆãŸã ã—å…¨éƒ¨ã‚¼ãƒ­ãªã‚‰ãƒˆãƒƒãƒ—ã‚’è¿”ã™ï¼‰
    if scored and scored[0][0] <= 1e-8:
        return scored[:topk]
    return [x for x in scored[:topk] if x[0] > 1e-8] or scored[:topk]


# ----------------------------
# UI
# ----------------------------
st.set_page_config(page_title="OCR RAG (Local)", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“„ OCR RAGï¼ˆãƒ­ãƒ¼ã‚«ãƒ«OCRâ†’JSONâ†’æ¤œç´¢ï¼‰")

with st.sidebar:
    st.header("è¨­å®š")
    st.write("æ¤œç´¢å¯¾è±¡ï¼š `data/ocr_results/*.json`")
    
    # PDF/ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨OCRå®Ÿè¡Œæ©Ÿèƒ½
    st.subheader("ğŸ“„ PDF/ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦OCR")
    ocr_available, ocr_msg = check_tesseract_available()
    
    if not ocr_available:
        st.warning(f"âš ï¸ OCRæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“: {ocr_msg}")
        st.info("ğŸ’¡ ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:\n- Tesseract OCR\n- Poppler (PDFç”¨)\n- pip install pillow pdf2image pytesseract")
    else:
        st.success(f"âœ… {ocr_msg}")
        
        uploaded_pdf = st.file_uploader(
            "PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=["pdf"],
            help="PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦OCRã‚’å®Ÿè¡Œã—ã¾ã™"
        )
        
        uploaded_image = st.file_uploader(
            "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=["png", "jpg", "jpeg", "tiff", "tif"],
            help="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦OCRã‚’å®Ÿè¡Œã—ã¾ã™"
        )
        
        if uploaded_pdf or uploaded_image:
            # OCRè¨­å®š
            with st.expander("OCRè¨­å®š", expanded=False):
                dpi = st.number_input("DPI (PDFç”¨)", min_value=100, max_value=600, value=200, step=50)
                lang = st.selectbox("è¨€èª", ["jpn", "jpn+eng", "eng"], index=1)
                psm = st.number_input("PSM (Page Segmentation Mode)", min_value=0, max_value=13, value=6)
                oem = st.number_input("OEM (OCR Engine Mode)", min_value=0, max_value=3, value=3)
            
            if st.button("ğŸš€ OCRå®Ÿè¡Œ", type="primary"):
                if uploaded_pdf:
                    with st.spinner("PDFã‚’å‡¦ç†ä¸­..."):
                        try:
                            pdf_bytes = uploaded_pdf.getvalue()
                            # å®¹é‡ãƒã‚§ãƒƒã‚¯
                            is_valid, msg = check_pdf_limits(pdf_bytes)
                            if not is_valid:
                                st.error(f"âŒ {msg}")
                            else:
                                st.info(f"ğŸ“„ {msg}")
                                
                                progress_bar = st.progress(0)
                                status_text = st.empty()
                                
                                def progress_callback(current, total):
                                    progress_bar.progress(current / total)
                                    status_text.text(f"å‡¦ç†ä¸­: {current}/{total}ãƒšãƒ¼ã‚¸")
                                
                                result = process_pdf_upload(
                                    pdf_bytes, uploaded_pdf.name, dpi=dpi, 
                                    lang=lang, psm=psm, oem=oem,
                                    progress_callback=progress_callback
                                )
                                
                                # JSONã¨ã—ã¦ä¿å­˜
                                json_filename = f"{Path(uploaded_pdf.name).stem}.json"
                                save_path = OCR_RESULTS_DIR / json_filename
                                OCR_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
                                
                                with save_path.open("w", encoding="utf-8") as f:
                                    json.dump(result, f, ensure_ascii=False, indent=2)
                                
                                progress_bar.empty()
                                status_text.empty()
                                st.success(f"âœ… OCRå®Œäº†: {len(result['pages'])}ãƒšãƒ¼ã‚¸ â†’ {json_filename}")
                                
                                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                                if "chunks" in st.session_state:
                                    del st.session_state["chunks"]
                        except Exception as e:
                            st.error(f"âŒ OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            import traceback
                            st.code(traceback.format_exc())
                
                if uploaded_image:
                    with st.spinner("ç”»åƒã‚’å‡¦ç†ä¸­..."):
                        try:
                            img_bytes = uploaded_image.getvalue()
                            result = process_image_upload(
                                img_bytes, uploaded_image.name,
                                lang=lang, psm=psm, oem=oem
                            )
                            
                            # JSONã¨ã—ã¦ä¿å­˜
                            json_filename = f"{Path(uploaded_image.name).stem}.json"
                            save_path = OCR_RESULTS_DIR / json_filename
                            OCR_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
                            
                            with save_path.open("w", encoding="utf-8") as f:
                                json.dump(result, f, ensure_ascii=False, indent=2)
                            
                            st.success(f"âœ… OCRå®Œäº†: {json_filename}")
                            
                            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                            if "chunks" in st.session_state:
                                del st.session_state["chunks"]
                        except Exception as e:
                            st.error(f"âŒ OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            import traceback
                            st.code(traceback.format_exc())
    
    st.divider()
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
    st.subheader("ğŸ“¤ JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "OCRçµæœã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["json"],
        accept_multiple_files=True,
        help="local_ocr_to_json.pyã§ç”Ÿæˆã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    if uploaded_files:
        OCR_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        saved_count = 0
        for uploaded_file in uploaded_files:
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
                if file_size_mb > MAX_JSON_FILE_SIZE_MB:
                    st.warning(f"âš ï¸ {uploaded_file.name} ã¯ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ ({file_size_mb:.1f}MB > {MAX_JSON_FILE_SIZE_MB}MB)")
                    continue
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                save_path = OCR_RESULTS_DIR / uploaded_file.name
                with save_path.open("wb") as f:
                    f.write(uploaded_file.getvalue())
                saved_count += 1
                st.success(f"âœ… {uploaded_file.name} ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({file_size_mb:.1f}MB)")
            except Exception as e:
                st.error(f"âŒ {uploaded_file.name} ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        if saved_count > 0:
            st.info(f"â„¹ï¸ {saved_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ã€ŒJSONã‚’å†èª­ã¿è¾¼ã¿ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†èª­ã¿è¾¼ã¿ã‚’ä¿ƒã™
            if "chunks" in st.session_state:
                del st.session_state["chunks"]
    
    st.divider()
    
    topk = st.slider("è¡¨ç¤ºä»¶æ•° (Top-K)", 1, 10, DEFAULT_TOPK)
    chunk_size = st.number_input("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", min_value=300, max_value=3000, value=900, step=100)
    overlap = st.number_input("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", min_value=0, max_value=500, value=150, step=10)
    reload_btn = st.button("ğŸ”„ JSONã‚’å†èª­ã¿è¾¼ã¿")

# èª­ã¿è¾¼ã¿
if "chunks" not in st.session_state or reload_btn:
    with st.spinner("JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        try:
            raw_chunks = load_all_chunks(OCR_RESULTS_DIR)

            # ãƒãƒ£ãƒ³ã‚¯è¨­å®šå¤‰æ›´ã«å¯¾å¿œï¼ˆå†åˆ†å‰²ã—ãŸã„ï¼‰
            # ã„ã£ãŸã‚“ pages ã‚’èª­ã¿ç›´ã—ã¦ä½œã‚Šç›´ã™
            rebuilt: List[Chunk] = []
            skipped_count = 0
            
            json_files = sorted(OCR_RESULTS_DIR.glob("*.json"))
            progress_bar = st.progress(0)
            for idx, jp in enumerate(json_files):
                try:
                    pages = load_ocr_json(jp)
                    new_chunks = make_chunks(jp.stem, jp.name, pages, chunk_size=int(chunk_size), overlap=int(overlap))
                    
                    # ç·ãƒãƒ£ãƒ³ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
                    if len(rebuilt) + len(new_chunks) > MAX_TOTAL_CHUNKS:
                        st.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šé™ã«é”ã—ãŸãŸã‚ã€{jp.name} ä»¥é™ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                        skipped_count = len(json_files) - idx
                        break
                    
                    rebuilt.extend(new_chunks)
                    progress_bar.progress((idx + 1) / len(json_files))
                except (ValueError, MemoryError) as e:
                    st.warning(f"âš ï¸ {jp.name} ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {e}")
                    skipped_count += 1
                    continue
            
            progress_bar.empty()
            st.session_state["chunks"] = rebuilt
            
            if skipped_count > 0:
                st.info(f"â„¹ï¸ {skipped_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚å®¹é‡ä¸Šé™ã‚’èª¿æ•´ã™ã‚‹å ´åˆã¯ config.py ã‚’ç·¨é›†ã—ã¦ãã ã•ã„ã€‚")
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.session_state["chunks"] = []

chunks: List[Chunk] = st.session_state["chunks"]

st.caption(f"èª­ã¿è¾¼ã¿JSONæ•°: {len(list(OCR_RESULTS_DIR.glob('*.json')))} / ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

if not chunks:
    st.warning("`data/ocr_results` ã« JSON ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.info("""
    **JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹æ–¹æ³•ï¼š**
    
    1. **ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰**
       - å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒğŸ“¤ JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    
    2. **ãƒ­ãƒ¼ã‚«ãƒ«ã§OCRã‚’å®Ÿè¡Œ**
       - `local_ocr_to_json.py` ã‚’ä½¿ã£ã¦PDF/ç”»åƒã‚’OCRã—ã€JSONã‚’ç”Ÿæˆ
       - ç”Ÿæˆã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ `data/ocr_results/` ã«é…ç½®
    
    3. **GitHubã«é…ç½®ï¼ˆStreamlit Cloudã®å ´åˆï¼‰**
       - JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ `data/ocr_results/` ã«é…ç½®ã—ã¦GitHubã«ãƒ—ãƒƒã‚·ãƒ¥
    """)
    st.stop()

query = st.text_input("æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šææ–™ / å®šå“¡ / 5052 / ã‚¢ãƒ«ãƒŸãƒ‹ã‚¦ãƒ ï¼‰", value="å®šå“¡")
go = st.button("ğŸ” æ¤œç´¢")

if go:
    results = search_chunks(chunks, query=query, topk=topk)

    st.subheader("æ¤œç´¢çµæœ")
    if not results:
        st.info("è©²å½“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¹ã‚³ã‚¢ãŒå…¨ã¦ã‚¼ãƒ­ï¼‰ã€‚åˆ¥ã®èªã‚„è¡¨è¨˜ã‚†ã‚Œã§è©¦ã—ã¦ãã ã•ã„ã€‚")
    else:
        for score, c in results:
            header = f"Score: {score:.4f} | File: {c.source_file} | Doc: {c.doc_id}"
            if c.page is not None:
                header += f" | Page: {c.page}"
            with st.expander(header, expanded=False):
                # ã‚¯ã‚¨ãƒªã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                t = c.text
                if query.strip():
                    t = re.sub(re.escape(query.strip()), lambda m: f"**{m.group(0)}**", t)
                st.write(t)
