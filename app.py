
# app.py ï¼ˆå®‰å®šç‰ˆï¼šCloudã¯æ¤œç´¢å°‚ç”¨ï¼‰
from __future__ import annotations
import io, json, os
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import streamlit as st

# ---- ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰ã¨ChromaDB ----
EMBED_OK = True
EMBED_ERR = ""
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
except Exception as e:
    EMBED_OK = False
    EMBED_ERR = str(e)
    SentenceTransformer = None
    np = None

CHROMA_OK = True
CHROMA_ERR = ""
try:
    import chromadb
except Exception as e:
    CHROMA_OK = False
    CHROMA_ERR = str(e)

# ---- ãƒšãƒ¼ã‚¸è¨­å®š ----
PAGE_TITLE = "æŠ€è¡“è³‡æ–™ OCRãƒ»RAG æ¤œç´¢ï¼ˆCloudï¼šæ¤œç´¢å°‚ç”¨ï¼‰"
PAGE_ICON = "ğŸ“„"
st.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON, layout="wide")

# ---- ãƒ‡ãƒ¼ã‚¿æ§‹é€  ----
@dataclass
class Doc:
    id: str
    title: str
    source: str
    text: str
    uploaded_at: str

# ---- ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼ˆè¾æ›¸ã‚¢ã‚¯ã‚»ã‚¹ã§å®‰å…¨ã«ï¼‰----
if "docs" not in st.session_state:
    st.session_state["docs"]: List[Doc] = []
if "index" not in st.session_state:
    st.session_state["index"] = None
if "doc_title" not in st.session_state:
    st.session_state["doc_title"] = None

# ---- JSONèª­ã¿è¾¼ã¿ ----
def load_json_from_upload(file) -> Optional[Dict]:
    try:
        return json.load(io.BytesIO(file.getvalue()))
    except Exception as e:
        st.error(f"JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_json_from_url(url: str) -> Optional[Dict]:
    try:
        import requests
        resp = requests.get(url, timeout=20)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        st.error(f"URLãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None

# ---- ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ï¼ˆlocal_ocr_to_json.py ã®å‡ºåŠ›ã«åˆã‚ã›ã‚‹ï¼‰----
def validate_schema(doc: Dict) -> bool:
    # è¨±å®¹ãƒˆãƒƒãƒ—ã‚­ãƒ¼ï¼šdoc_id/title/source/created_at/pages
    if not isinstance(doc, dict):
        st.error("JSONã®ãƒˆãƒƒãƒ—ãŒè¾æ›¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"); return False
    for k in ["pages"]:
        if k not in doc:
            st.error(f"ã‚­ãƒ¼ãŒä¸è¶³: {k}"); return False
    if not isinstance(doc["pages"], list) or len(doc["pages"]) == 0:
        st.error("pages ãŒç©ºã§ã™ã€‚"); return False
    # pages[*].page_num ã¨ text ã‚’å¿…é ˆåŒ–
    for p in doc["pages"]:
        if "text" not in p:
            st.error("pages[*].text ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return False
        if "page_num" not in p and "page" not in p:
            st.error("pages[*].page_numï¼ˆã¾ãŸã¯ pageï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return False
    return True

# ---- ãƒ†ã‚­ã‚¹ãƒˆçµåˆ ----
def join_text(doc: Dict) -> str:
    buf = []
    for p in doc["pages"]:
        page_no = p.get("page_num", p.get("page", None))
        t = p.get("text", "")
        buf.append(f"=== ãƒšãƒ¼ã‚¸ {page_no} ===\n{t}\n")
    return "\n".join(buf)

# ---- ãƒãƒ£ãƒ³ã‚¯åŒ– ----
def chunk_text(text: str, chunk_size=1000, overlap=200) -> List[str]:
    chunks = []
    n = len(text)
    i = 0
    while i < n:
        j = min(i + chunk_size, n)
        chunk = text[i:j].strip()
        if chunk:
            chunks.append(chunk)
        if j >= n:
            break
        i = max(0, j - overlap)
    return chunks

# ---- 1) TF-IDF ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªæ¤œç´¢ï¼ˆè¶…è»½é‡ï¼ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰----
class TfIdfIndex:
    def __init__(self):
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer()
        self.texts: List[str] = []
        self.metas: List[Dict] = []
        self.matrix = None

    def add(self, texts: List[str], metas: List[Dict]):
        self.texts.extend(texts)
        self.metas.extend(metas)
        self.matrix = self.vectorizer.fit_transform(self.texts)

    def search(self, query: str, top_k=5):
        if self.matrix is None or not self.texts:
            return []
        qv = self.vectorizer.transform([query])
        scores = (self.matrix @ qv.T).toarray().ravel()
        idx = scores.argsort()[::-1][:top_k]
        results = []
        for i in idx:
            results.append({
                "score": float(scores[i]),
                "text": self.texts[i],
                "meta": self.metas[i]
            })
        return results

# ---- 2) Sentence-Transformers æ¤œç´¢ï¼ˆè»½é‡åŸ‹ã‚è¾¼ã¿ï¼ä»»æ„ï¼‰----
class EmbeddingIndex:
    def __init__(self, model_name="paraphrase-multilingual-MiniLM-L12-v2"):
        if not EMBED_OK:
            raise RuntimeError(f"sentence-transformers ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {EMBED_ERR}")
        self.model = SentenceTransformer(model_name)
        self.texts: List[str] = []
        self.metas: List[Dict] = []
        self.embeds = None  # np.ndarray

    def add(self, texts: List[str], metas: List[Dict]):
        emb = self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
        if self.embeds is None:
            self.embeds = emb
        else:
            self.embeds = np.vstack([self.embeds, emb])
        self.texts.extend(texts)
        self.metas.extend(metas)

    def search(self, query: str, top_k=5):
        q = self.model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
        sims = (self.embeds @ q.T).ravel()
        idx = sims.argsort()[::-1][:top_k]
        return [{
            "score": float(sims[i]),
            "text": self.texts[i],
            "meta": self.metas[i]
        } for i in idx]

# ---- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆãƒ¢ãƒ¼ãƒ‰é¸æŠï¼‰----
def build_index(doc: Dict, mode: str = "tfidf", chunk_size=1000, overlap=200):
    texts, metas = [], []
    merged = join_text(doc)
    for c in chunk_text(merged, chunk_size=chunk_size, overlap=overlap):
        texts.append(c)
        metas.append({
            "page_num": None,
            "title": doc.get("title") or doc.get("source") or "",
            "source": doc.get("source") or doc.get("doc_id") or "",
        })
    if mode == "tfidf":
        idx = TfIdfIndex()
        idx.add(texts, metas)
        return idx
    elif mode == "embed":
        idx = EmbeddingIndex(model_name=st.session_state.get("embed_model", "paraphrase-multilingual-MiniLM-L12-v2"))
        idx.add(texts, metas)
        return idx
    else:
        raise ValueError("mode ã¯ 'tfidf' ã‹ 'embed' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

# ---- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----
with st.sidebar:
    st.header("âš™ï¸ Cloudé‹ç”¨ãƒ¢ãƒ¼ãƒ‰")
    mode = st.radio("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["TFâ€‘IDFï¼ˆè»½é‡ãƒ»æ¨å¥¨ï¼‰", "åŸ‹ã‚è¾¼ã¿ï¼ˆSentenceâ€‘Transformersï¼‰"], index=0)
    top_k = st.slider("Topâ€‘K", 1, 10, 5)
    st.caption("â€» åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã¯åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
    if mode.startswith("åŸ‹ã‚è¾¼ã¿"):
        st.session_state["embed_model"] = st.selectbox(
            "ãƒ¢ãƒ‡ãƒ«", ["paraphrase-multilingual-MiniLM-L12-v2", "all-MiniLM-L6-v2"], index=0
        )
    st.markdown("---")
    st.subheader("RAGï¼ˆChromaDBï¼‰")
    if CHROMA_OK:
        st.caption("â€» ä»Šå›ã¯ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªæ¤œç´¢ã®ã¿ï¼ˆChromaDBã¯æœªä½¿ç”¨ï¼‰ã€‚å¿…è¦ãªã‚‰å¾Œã§è¿½åŠ å¯èƒ½ã€‚")
    else:
        st.warning(f"ChromaDB æœªä½¿ç”¨ï¼ˆç†ç”±: {CHROMA_ERR}ï¼‰")

# ---- ãƒ¡ã‚¤ãƒ³UI ----
st.title(f"{PAGE_ICON} {PAGE_TITLE}")
tab1, tab2, tab3 = st.tabs(["ğŸ“¤ JSONã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ğŸ”— URLã‹ã‚‰ãƒ­ãƒ¼ãƒ‰", "ğŸ” æ¤œç´¢"])

# 1) JSONã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
with tab1:
    st.header("ğŸ“¤ ãƒ­ãƒ¼ã‚«ãƒ«OCRç”Ÿæˆã® JSON ã‚’èª­ã¿è¾¼ã‚€")
    up = st.file_uploader("OCRçµæœJSONï¼ˆlocal_ocr_to_json.py ã®å‡ºåŠ›ï¼‰", type=["json"])
    if up:
        doc = load_json_from_upload(up)
        if doc and validate_schema(doc):
            idx_mode = "tfidf" if mode.startswith("TF") else "embed"
            with st.spinner("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­â€¦"):
                st.session_state["index"] = build_index(doc, mode=idx_mode, chunk_size=1000, overlap=200)
                st.session_state["doc_title"] = doc.get("title") or doc.get("source") or up.name
            st.success("èª­ã¿è¾¼ã¿å®Œäº†ã€‚æ¤œç´¢ã‚¿ãƒ–ã¸ã©ã†ãã€‚")

# 2) URLãƒ­ãƒ¼ãƒ‰
with tab2:
    st.header("ğŸ”— GitHub Raw ç­‰ã® URL ã‹ã‚‰èª­ã¿è¾¼ã¿")
    url = st.text_input("JSONã®URL")
    if st.button("URLã‹ã‚‰èª­ã¿è¾¼ã‚€") and url.strip():
        doc = load_json_from_url(url.strip())
        if doc and validate_schema(doc):
            idx_mode = "tfidf" if mode.startswith("TF") else "embed"
            with st.spinner("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­â€¦"):
                st.session_state["index"] = build_index(doc, mode=idx_mode, chunk_size=1000, overlap=200)
                st.session_state["doc_title"] = doc.get("title") or doc.get("source") or url
            st.success("èª­ã¿è¾¼ã¿å®Œäº†ã€‚æ¤œç´¢ã‚¿ãƒ–ã¸ã©ã†ãã€‚")

# 3) æ¤œç´¢
with tab3:
    st.header(f"ğŸ” æ¤œç´¢ï¼ˆ{st.session_state.get('doc_title') or 'æœªèª­è¾¼'}ï¼‰")
    if st.session_state.get("index") is None:
        st.info("ã¾ãš JSON ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
    else:
        q = st.text_input("è³ªå•ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        if st.button("æ¤œç´¢") and q.strip():
            try:
                results = st.session_state["index"].search(q.strip(), top_k=top_k)
                if results:
                    st.subheader(f"æ¤œç´¢çµæœï¼š{len(results)}ä»¶")
                    for i, r in enumerate(results, 1):
                        score = r.get("score")
                        with st.expander(f"çµæœ {i}ï¼ˆscoreâ‰ˆ{score:.3f}ï¼‰"):
                            st.write("**ãƒ¡ã‚¿**")
                            st.json(r.get("meta", {}))
                            st.write("**è©²å½“ãƒ†ã‚­ã‚¹ãƒˆï¼ˆchunkï¼‰**")
                            st.text(r["text"])
                else:
                    st.info("è©²å½“ãªã—ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„è¡¨è¨˜æºã‚Œã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
            except Exception as e:
                st.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


